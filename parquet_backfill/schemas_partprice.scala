var converterMap:HashMap[String,Any] = new HashMap()
class fc_feed_bristol_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_bristol(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_bristol = new fc_feed_bristol_converter()
converterMap+=("fc_feed_bristol"->dummyobj_fc_feed_bristol)
class fc_feed_standardelec_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_standardelec(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_standardelec = new fc_feed_standardelec_converter()
converterMap+=("fc_feed_standardelec"->dummyobj_fc_feed_standardelec)
class fc_feed_chip1stop_apac_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_chip1stop_apac(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_chip1stop_apac = new fc_feed_chip1stop_apac_converter()
converterMap+=("fc_feed_chip1stop_apac"->dummyobj_fc_feed_chip1stop_apac)
class fc_feed_detailtech_cn_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_detailtech_cn(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_detailtech_cn = new fc_feed_detailtech_cn_converter()
converterMap+=("fc_feed_detailtech_cn"->dummyobj_fc_feed_detailtech_cn)
class fc_feed_classic_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_classic(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_classic = new fc_feed_classic_converter()
converterMap+=("fc_feed_classic"->dummyobj_fc_feed_classic)
class fc_feed_farnell_bnl_jan_2021_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_farnell_bnl_jan_2021(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_farnell_bnl_jan_2021 = new fc_feed_farnell_bnl_jan_2021_converter()
converterMap+=("fc_feed_farnell_bnl_jan_2021"->dummyobj_fc_feed_farnell_bnl_jan_2021)
class fc_feed_heilind_oems_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_heilind_oems(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_heilind_oems = new fc_feed_heilind_oems_converter()
converterMap+=("fc_feed_heilind_oems"->dummyobj_fc_feed_heilind_oems)
class fc_feed_element14_rp_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_element14_rp_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_element14_rp_bnl = new fc_feed_element14_rp_bnl_converter()
converterMap+=("fc_feed_element14_rp_bnl"->dummyobj_fc_feed_element14_rp_bnl)
class fc_feed_mautronics_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_mautronics(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_mautronics = new fc_feed_mautronics_converter()
converterMap+=("fc_feed_mautronics"->dummyobj_fc_feed_mautronics)
class fc_feed_farnell_fr_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_farnell_fr(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_farnell_fr = new fc_feed_farnell_fr_converter()
converterMap+=("fc_feed_farnell_fr"->dummyobj_fc_feed_farnell_fr)
class fc_feed_newark_us_bnl2_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_newark_us_bnl2(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_newark_us_bnl2 = new fc_feed_newark_us_bnl2_converter()
converterMap+=("fc_feed_newark_us_bnl2"->dummyobj_fc_feed_newark_us_bnl2)
class fc_feed_origparts_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_origparts(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_origparts = new fc_feed_origparts_converter()
converterMap+=("fc_feed_origparts"->dummyobj_fc_feed_origparts)
class fc_feed_tme_bnl_fr_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_tme_bnl_fr(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_tme_bnl_fr = new fc_feed_tme_bnl_fr_converter()
converterMap+=("fc_feed_tme_bnl_fr"->dummyobj_fc_feed_tme_bnl_fr)
class fc_feed_kedlielec_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_kedlielec(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_kedlielec = new fc_feed_kedlielec_converter()
converterMap+=("fc_feed_kedlielec"->dummyobj_fc_feed_kedlielec)
class fc_feed_farnell_fi_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_farnell_fi(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_farnell_fi = new fc_feed_farnell_fi_converter()
converterMap+=("fc_feed_farnell_fi"->dummyobj_fc_feed_farnell_fi)
class fc_feed_rochester_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rochester_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rochester_bnl = new fc_feed_rochester_bnl_converter()
converterMap+=("fc_feed_rochester_bnl"->dummyobj_fc_feed_rochester_bnl)
class fc_feed_olc_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_olc(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_olc = new fc_feed_olc_converter()
converterMap+=("fc_feed_olc"->dummyobj_fc_feed_olc)
class fc_feed_galco_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_galco(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_galco = new fc_feed_galco_converter()
converterMap+=("fc_feed_galco"->dummyobj_fc_feed_galco)
class fc_feed_computer_controls_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_computer_controls(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_computer_controls = new fc_feed_computer_controls_converter()
converterMap+=("fc_feed_computer_controls"->dummyobj_fc_feed_computer_controls)
class fc_feed_farnell_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_farnell_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_farnell_bnl = new fc_feed_farnell_bnl_converter()
converterMap+=("fc_feed_farnell_bnl"->dummyobj_fc_feed_farnell_bnl)
class fc_feed_metaverse_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_metaverse(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_metaverse = new fc_feed_metaverse_converter()
converterMap+=("fc_feed_metaverse"->dummyobj_fc_feed_metaverse)
class fc_feed_icsoso_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_icsoso(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_icsoso = new fc_feed_icsoso_converter()
converterMap+=("fc_feed_icsoso"->dummyobj_fc_feed_icsoso)
class fc_feed_xituo_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_xituo(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_xituo = new fc_feed_xituo_converter()
converterMap+=("fc_feed_xituo"->dummyobj_fc_feed_xituo)
class fc_feed_chip1stop_usd_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_chip1stop_usd(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_chip1stop_usd = new fc_feed_chip1stop_usd_converter()
converterMap+=("fc_feed_chip1stop_usd"->dummyobj_fc_feed_chip1stop_usd)
class fc_feed_distrelec_ro_findchips_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_distrelec_ro_findchips(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_distrelec_ro_findchips = new fc_feed_distrelec_ro_findchips_converter()
converterMap+=("fc_feed_distrelec_ro_findchips"->dummyobj_fc_feed_distrelec_ro_findchips)
class fc_feed_chip1stop_cny_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_chip1stop_cny(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_chip1stop_cny = new fc_feed_chip1stop_cny_converter()
converterMap+=("fc_feed_chip1stop_cny"->dummyobj_fc_feed_chip1stop_cny)
class fc_feed_comsit_fc_it_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_comsit_fc_it(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_comsit_fc_it = new fc_feed_comsit_fc_it_converter()
converterMap+=("fc_feed_comsit_fc_it"->dummyobj_fc_feed_comsit_fc_it)
class fc_feed_distrelec_nl_findchips_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_distrelec_nl_findchips(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_distrelec_nl_findchips = new fc_feed_distrelec_nl_findchips_converter()
converterMap+=("fc_feed_distrelec_nl_findchips"->dummyobj_fc_feed_distrelec_nl_findchips)
class fc_feed_future_china_fc_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_future_china_fc(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_future_china_fc = new fc_feed_future_china_fc_converter()
converterMap+=("fc_feed_future_china_fc"->dummyobj_fc_feed_future_china_fc)
class fc_feed_sierraic_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_sierraic(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_sierraic = new fc_feed_sierraic_converter()
converterMap+=("fc_feed_sierraic"->dummyobj_fc_feed_sierraic)
class fc_feed_sense_elec_fc_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_sense_elec_fc(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_sense_elec_fc = new fc_feed_sense_elec_fc_converter()
converterMap+=("fc_feed_sense_elec_fc"->dummyobj_fc_feed_sense_elec_fc)
class fc_feed_tme_bnl_fi_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_tme_bnl_fi(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_tme_bnl_fi = new fc_feed_tme_bnl_fi_converter()
converterMap+=("fc_feed_tme_bnl_fi"->dummyobj_fc_feed_tme_bnl_fi)
class fc_feed_moseley_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_moseley(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_moseley = new fc_feed_moseley_converter()
converterMap+=("fc_feed_moseley"->dummyobj_fc_feed_moseley)
class fc_feed_element14_ph_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_element14_ph(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_element14_ph = new fc_feed_element14_ph_converter()
converterMap+=("fc_feed_element14_ph"->dummyobj_fc_feed_element14_ph)
class fc_feed_rs_components_tr_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_tr(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_tr = new fc_feed_rs_components_tr_converter()
converterMap+=("fc_feed_rs_components_tr"->dummyobj_fc_feed_rs_components_tr)
class fc_feed_comsit_fc_de_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_comsit_fc_de(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_comsit_fc_de = new fc_feed_comsit_fc_de_converter()
converterMap+=("fc_feed_comsit_fc_de"->dummyobj_fc_feed_comsit_fc_de)
class fc_feed_microworks_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_microworks(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_microworks = new fc_feed_microworks_converter()
converterMap+=("fc_feed_microworks"->dummyobj_fc_feed_microworks)
class fc_feed_distrelec_pl_oemstrade_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_distrelec_pl_oemstrade(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_distrelec_pl_oemstrade = new fc_feed_distrelec_pl_oemstrade_converter()
converterMap+=("fc_feed_distrelec_pl_oemstrade"->dummyobj_fc_feed_distrelec_pl_oemstrade)
class fc_feed_comsit_fc_in_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_comsit_fc_in(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_comsit_fc_in = new fc_feed_comsit_fc_in_converter()
converterMap+=("fc_feed_comsit_fc_in"->dummyobj_fc_feed_comsit_fc_in)
class fc_feed_chip1stop_cny_multi_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_chip1stop_cny_multi(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_chip1stop_cny_multi = new fc_feed_chip1stop_cny_multi_converter()
converterMap+=("fc_feed_chip1stop_cny_multi"->dummyobj_fc_feed_chip1stop_cny_multi)
class fc_feed_farnell_dk_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_farnell_dk(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_farnell_dk = new fc_feed_farnell_dk_converter()
converterMap+=("fc_feed_farnell_dk"->dummyobj_fc_feed_farnell_dk)
class fc_feed_xinghuan_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_xinghuan(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_xinghuan = new fc_feed_xinghuan_converter()
converterMap+=("fc_feed_xinghuan"->dummyobj_fc_feed_xinghuan)
class fc_feed_analog_devices_precision_adc_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_analog_devices_precision_adc(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_analog_devices_precision_adc = new fc_feed_analog_devices_precision_adc_converter()
converterMap+=("fc_feed_analog_devices_precision_adc"->dummyobj_fc_feed_analog_devices_precision_adc)
class fc_feed_jak_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_jak(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_jak = new fc_feed_jak_converter()
converterMap+=("fc_feed_jak"->dummyobj_fc_feed_jak)
class fc_feed_iccomp_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_iccomp(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_iccomp = new fc_feed_iccomp_converter()
converterMap+=("fc_feed_iccomp"->dummyobj_fc_feed_iccomp)
class fc_feed_northstar_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_northstar(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_northstar = new fc_feed_northstar_converter()
converterMap+=("fc_feed_northstar"->dummyobj_fc_feed_northstar)
class fc_feed_ameya_fc_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_ameya_fc_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_ameya_fc_bnl = new fc_feed_ameya_fc_bnl_converter()
converterMap+=("fc_feed_ameya_fc_bnl"->dummyobj_fc_feed_ameya_fc_bnl)
class fc_feed_rs_components_cz_mft_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_cz_mft_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_cz_mft_bnl = new fc_feed_rs_components_cz_mft_bnl_converter()
converterMap+=("fc_feed_rs_components_cz_mft_bnl"->dummyobj_fc_feed_rs_components_cz_mft_bnl)
class fc_feed_comsit_fc_us_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_comsit_fc_us(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_comsit_fc_us = new fc_feed_comsit_fc_us_converter()
converterMap+=("fc_feed_comsit_fc_us"->dummyobj_fc_feed_comsit_fc_us)
class fc_feed_compexpert_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_compexpert(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_compexpert = new fc_feed_compexpert_converter()
converterMap+=("fc_feed_compexpert"->dummyobj_fc_feed_compexpert)
class fc_feed_ickey_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_ickey_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_ickey_bnl = new fc_feed_ickey_bnl_converter()
converterMap+=("fc_feed_ickey_bnl"->dummyobj_fc_feed_ickey_bnl)
class fc_feed_rs_components_hu_mft_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_hu_mft_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_hu_mft_bnl = new fc_feed_rs_components_hu_mft_bnl_converter()
converterMap+=("fc_feed_rs_components_hu_mft_bnl"->dummyobj_fc_feed_rs_components_hu_mft_bnl)
class fc_feed_rs_americas_bnl_now_stocking_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_americas_bnl_now_stocking(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_americas_bnl_now_stocking = new fc_feed_rs_americas_bnl_now_stocking_converter()
converterMap+=("fc_feed_rs_americas_bnl_now_stocking"->dummyobj_fc_feed_rs_americas_bnl_now_stocking)
class fc_feed_peigenesis_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_peigenesis(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_peigenesis = new fc_feed_peigenesis_converter()
converterMap+=("fc_feed_peigenesis"->dummyobj_fc_feed_peigenesis)
class fc_feed_peigenesis_004_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_peigenesis_004_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_peigenesis_004_bnl = new fc_feed_peigenesis_004_bnl_converter()
converterMap+=("fc_feed_peigenesis_004_bnl"->dummyobj_fc_feed_peigenesis_004_bnl)
class fc_feed_quotebeam_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_quotebeam(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_quotebeam = new fc_feed_quotebeam_converter()
converterMap+=("fc_feed_quotebeam"->dummyobj_fc_feed_quotebeam)
class fc_feed_rs_components_apac_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_apac(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_apac = new fc_feed_rs_components_apac_converter()
converterMap+=("fc_feed_rs_components_apac"->dummyobj_fc_feed_rs_components_apac)
class fc_feed_jgm_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_jgm(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_jgm = new fc_feed_jgm_converter()
converterMap+=("fc_feed_jgm"->dummyobj_fc_feed_jgm)
class fc_feed_chip1stop_usd_oems_3_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_chip1stop_usd_oems_3(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_chip1stop_usd_oems_3 = new fc_feed_chip1stop_usd_oems_3_converter()
converterMap+=("fc_feed_chip1stop_usd_oems_3"->dummyobj_fc_feed_chip1stop_usd_oems_3)
class fc_feed_future_bnl_optimized_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_future_bnl_optimized(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_future_bnl_optimized = new fc_feed_future_bnl_optimized_converter()
converterMap+=("fc_feed_future_bnl_optimized"->dummyobj_fc_feed_future_bnl_optimized)
class fc_feed_rs_components_sg_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_sg(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_sg = new fc_feed_rs_components_sg_converter()
converterMap+=("fc_feed_rs_components_sg"->dummyobj_fc_feed_rs_components_sg)
class fc_feed_hklilin_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_hklilin(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_hklilin = new fc_feed_hklilin_converter()
converterMap+=("fc_feed_hklilin"->dummyobj_fc_feed_hklilin)
class fc_feed_rspro_ch_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rspro_ch(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rspro_ch = new fc_feed_rspro_ch_converter()
converterMap+=("fc_feed_rspro_ch"->dummyobj_fc_feed_rspro_ch)
class fc_feed_ti_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_ti(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_ti = new fc_feed_ti_converter()
converterMap+=("fc_feed_ti"->dummyobj_fc_feed_ti)
class fc_feed_rspro_cz_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rspro_cz(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rspro_cz = new fc_feed_rspro_cz_converter()
converterMap+=("fc_feed_rspro_cz"->dummyobj_fc_feed_rspro_cz)
class fc_feed_rochester_cn_ti_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rochester_cn_ti_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rochester_cn_ti_bnl = new fc_feed_rochester_cn_ti_bnl_converter()
converterMap+=("fc_feed_rochester_cn_ti_bnl"->dummyobj_fc_feed_rochester_cn_ti_bnl)
class fc_feed_farnell_no_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_farnell_no(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_farnell_no = new fc_feed_farnell_no_converter()
converterMap+=("fc_feed_farnell_no"->dummyobj_fc_feed_farnell_no)
class fc_feed_semtke_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_semtke(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_semtke = new fc_feed_semtke_converter()
converterMap+=("fc_feed_semtke"->dummyobj_fc_feed_semtke)
class fc_feed_rapid_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rapid(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rapid = new fc_feed_rapid_converter()
converterMap+=("fc_feed_rapid"->dummyobj_fc_feed_rapid)
class fc_feed_digikey_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_digikey_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_digikey_bnl = new fc_feed_digikey_bnl_converter()
converterMap+=("fc_feed_digikey_bnl"->dummyobj_fc_feed_digikey_bnl)
class fc_feed_avnet_europe_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_avnet_europe(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_avnet_europe = new fc_feed_avnet_europe_converter()
converterMap+=("fc_feed_avnet_europe"->dummyobj_fc_feed_avnet_europe)
class fc_feed_splendent_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_splendent(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_splendent = new fc_feed_splendent_converter()
converterMap+=("fc_feed_splendent"->dummyobj_fc_feed_splendent)
class fc_feed_digikey_oems_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_digikey_oems_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_digikey_oems_bnl = new fc_feed_digikey_oems_bnl_converter()
converterMap+=("fc_feed_digikey_oems_bnl"->dummyobj_fc_feed_digikey_oems_bnl)
class fc_feed_digikey_uk_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_digikey_uk(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_digikey_uk = new fc_feed_digikey_uk_converter()
converterMap+=("fc_feed_digikey_uk"->dummyobj_fc_feed_digikey_uk)
class fc_feed_breizelec_oems_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_breizelec_oems(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_breizelec_oems = new fc_feed_breizelec_oems_converter()
converterMap+=("fc_feed_breizelec_oems"->dummyobj_fc_feed_breizelec_oems)
class fc_feed_rs_components_be_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_be(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_be = new fc_feed_rs_components_be_converter()
converterMap+=("fc_feed_rs_components_be"->dummyobj_fc_feed_rs_components_be)
class fc_feed_b2b_rs_components_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_b2b_rs_components(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_b2b_rs_components = new fc_feed_b2b_rs_components_converter()
converterMap+=("fc_feed_b2b_rs_components"->dummyobj_fc_feed_b2b_rs_components)
class fc_feed_futuretech_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_futuretech(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_futuretech = new fc_feed_futuretech_converter()
converterMap+=("fc_feed_futuretech"->dummyobj_fc_feed_futuretech)
class fc_feed_rutronik_us_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rutronik_us(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rutronik_us = new fc_feed_rutronik_us_converter()
converterMap+=("fc_feed_rutronik_us"->dummyobj_fc_feed_rutronik_us)
class fc_feed_rspro_se_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rspro_se(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rspro_se = new fc_feed_rspro_se_converter()
converterMap+=("fc_feed_rspro_se"->dummyobj_fc_feed_rspro_se)
class fc_feed_element14_b2b_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_element14_b2b(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_element14_b2b = new fc_feed_element14_b2b_converter()
converterMap+=("fc_feed_element14_b2b"->dummyobj_fc_feed_element14_b2b)
class fc_feed_ardusimple_fc_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_ardusimple_fc(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_ardusimple_fc = new fc_feed_ardusimple_fc_converter()
converterMap+=("fc_feed_ardusimple_fc"->dummyobj_fc_feed_ardusimple_fc)
class fc_feed_farnell_rohde_bnl_fr_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_farnell_rohde_bnl_fr(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_farnell_rohde_bnl_fr = new fc_feed_farnell_rohde_bnl_fr_converter()
converterMap+=("fc_feed_farnell_rohde_bnl_fr"->dummyobj_fc_feed_farnell_rohde_bnl_fr)
class fc_feed_tme_bnl_es_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_tme_bnl_es(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_tme_bnl_es = new fc_feed_tme_bnl_es_converter()
converterMap+=("fc_feed_tme_bnl_es"->dummyobj_fc_feed_tme_bnl_es)
class fc_feed_rs_components_pl_mft_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_pl_mft_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_pl_mft_bnl = new fc_feed_rs_components_pl_mft_bnl_converter()
converterMap+=("fc_feed_rs_components_pl_mft_bnl"->dummyobj_fc_feed_rs_components_pl_mft_bnl)
class fc_feed_integ_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_integ(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_integ = new fc_feed_integ_converter()
converterMap+=("fc_feed_integ"->dummyobj_fc_feed_integ)
class fc_feed_farnell_es_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_farnell_es(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_farnell_es = new fc_feed_farnell_es_converter()
converterMap+=("fc_feed_farnell_es"->dummyobj_fc_feed_farnell_es)
class fc_feed_element14_hp_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_element14_hp_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_element14_hp_bnl = new fc_feed_element14_hp_bnl_converter()
converterMap+=("fc_feed_element14_hp_bnl"->dummyobj_fc_feed_element14_hp_bnl)
class fc_feed_distrelec_pl_findchips_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_distrelec_pl_findchips(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_distrelec_pl_findchips = new fc_feed_distrelec_pl_findchips_converter()
converterMap+=("fc_feed_distrelec_pl_findchips"->dummyobj_fc_feed_distrelec_pl_findchips)
class fc_feed_sager_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_sager(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_sager = new fc_feed_sager_converter()
converterMap+=("fc_feed_sager"->dummyobj_fc_feed_sager)
class fc_feed_tme_bnl_de_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_tme_bnl_de(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_tme_bnl_de = new fc_feed_tme_bnl_de_converter()
converterMap+=("fc_feed_tme_bnl_de"->dummyobj_fc_feed_tme_bnl_de)
class fc_feed_ryx_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_ryx(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_ryx = new fc_feed_ryx_converter()
converterMap+=("fc_feed_ryx"->dummyobj_fc_feed_ryx)
class fc_feed_ampheo_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_ampheo(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_ampheo = new fc_feed_ampheo_converter()
converterMap+=("fc_feed_ampheo"->dummyobj_fc_feed_ampheo)
class fc_feed_rutronik_euro_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rutronik_euro(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rutronik_euro = new fc_feed_rutronik_euro_converter()
converterMap+=("fc_feed_rutronik_euro"->dummyobj_fc_feed_rutronik_euro)
class fc_feed_chip1stop_eur_oems_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_chip1stop_eur_oems(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_chip1stop_eur_oems = new fc_feed_chip1stop_eur_oems_converter()
converterMap+=("fc_feed_chip1stop_eur_oems"->dummyobj_fc_feed_chip1stop_eur_oems)
class fc_feed_oneyac_fc_china_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_oneyac_fc_china(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_oneyac_fc_china = new fc_feed_oneyac_fc_china_converter()
converterMap+=("fc_feed_oneyac_fc_china"->dummyobj_fc_feed_oneyac_fc_china)
class fc_feed_newark_bnl_littelfuse_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_newark_bnl_littelfuse(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_newark_bnl_littelfuse = new fc_feed_newark_bnl_littelfuse_converter()
converterMap+=("fc_feed_newark_bnl_littelfuse"->dummyobj_fc_feed_newark_bnl_littelfuse)
class fc_feed_chip1stop_eur_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_chip1stop_eur(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_chip1stop_eur = new fc_feed_chip1stop_eur_converter()
converterMap+=("fc_feed_chip1stop_eur"->dummyobj_fc_feed_chip1stop_eur)
class fc_feed_hengshi_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_hengshi(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_hengshi = new fc_feed_hengshi_converter()
converterMap+=("fc_feed_hengshi"->dummyobj_fc_feed_hengshi)
class fc_feed_directcomp_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_directcomp(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_directcomp = new fc_feed_directcomp_converter()
converterMap+=("fc_feed_directcomp"->dummyobj_fc_feed_directcomp)
class fc_feed_asc_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_asc(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_asc = new fc_feed_asc_converter()
converterMap+=("fc_feed_asc"->dummyobj_fc_feed_asc)
class fc_feed_utmel_fc_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_utmel_fc(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_utmel_fc = new fc_feed_utmel_fc_converter()
converterMap+=("fc_feed_utmel_fc"->dummyobj_fc_feed_utmel_fc)
class fc_feed_ezkey_fc_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_ezkey_fc(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_ezkey_fc = new fc_feed_ezkey_fc_converter()
converterMap+=("fc_feed_ezkey_fc"->dummyobj_fc_feed_ezkey_fc)
class fc_feed_stmicro_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_stmicro(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_stmicro = new fc_feed_stmicro_converter()
converterMap+=("fc_feed_stmicro"->dummyobj_fc_feed_stmicro)
class fc_feed_rs_components_uk_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_uk(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_uk = new fc_feed_rs_components_uk_converter()
converterMap+=("fc_feed_rs_components_uk"->dummyobj_fc_feed_rs_components_uk)
class fc_feed_silitech_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_silitech(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_silitech = new fc_feed_silitech_converter()
converterMap+=("fc_feed_silitech"->dummyobj_fc_feed_silitech)
class fc_feed_rochester_jp_ti_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rochester_jp_ti_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rochester_jp_ti_bnl = new fc_feed_rochester_jp_ti_bnl_converter()
converterMap+=("fc_feed_rochester_jp_ti_bnl"->dummyobj_fc_feed_rochester_jp_ti_bnl)
class fc_feed_rs_components_fi_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_fi(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_fi = new fc_feed_rs_components_fi_converter()
converterMap+=("fc_feed_rs_components_fi"->dummyobj_fc_feed_rs_components_fi)
class fc_feed_chipmall_fc_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_chipmall_fc(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_chipmall_fc = new fc_feed_chipmall_fc_converter()
converterMap+=("fc_feed_chipmall_fc"->dummyobj_fc_feed_chipmall_fc)
class fc_feed_ozdisan_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_ozdisan(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_ozdisan = new fc_feed_ozdisan_converter()
converterMap+=("fc_feed_ozdisan"->dummyobj_fc_feed_ozdisan)
class fc_feed_winshare_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_winshare(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_winshare = new fc_feed_winshare_converter()
converterMap+=("fc_feed_winshare"->dummyobj_fc_feed_winshare)
class fc_feed_mygroup_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_mygroup(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_mygroup = new fc_feed_mygroup_converter()
converterMap+=("fc_feed_mygroup"->dummyobj_fc_feed_mygroup)
class fc_feed_ysonix_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_ysonix(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_ysonix = new fc_feed_ysonix_converter()
converterMap+=("fc_feed_ysonix"->dummyobj_fc_feed_ysonix)
class fc_feed_ti_bnl_dlp_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_ti_bnl_dlp(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_ti_bnl_dlp = new fc_feed_ti_bnl_dlp_converter()
converterMap+=("fc_feed_ti_bnl_dlp"->dummyobj_fc_feed_ti_bnl_dlp)
class fc_feed_testequity_ukie_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_testequity_ukie(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_testequity_ukie = new fc_feed_testequity_ukie_converter()
converterMap+=("fc_feed_testequity_ukie"->dummyobj_fc_feed_testequity_ukie)
class fc_feed_newadantage_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_newadantage_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_newadantage_bnl = new fc_feed_newadantage_bnl_converter()
converterMap+=("fc_feed_newadantage_bnl"->dummyobj_fc_feed_newadantage_bnl)
class fc_feed_transtector_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_transtector(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_transtector = new fc_feed_transtector_converter()
converterMap+=("fc_feed_transtector"->dummyobj_fc_feed_transtector)
class fc_feed_techdesign_oems_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_techdesign_oems(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_techdesign_oems = new fc_feed_techdesign_oems_converter()
converterMap+=("fc_feed_techdesign_oems"->dummyobj_fc_feed_techdesign_oems)
class fc_feed_heilind_bnl_te_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_heilind_bnl_te(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_heilind_bnl_te = new fc_feed_heilind_bnl_te_converter()
converterMap+=("fc_feed_heilind_bnl_te"->dummyobj_fc_feed_heilind_bnl_te)
class fc_feed_rs_components_fr_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_fr(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_fr = new fc_feed_rs_components_fr_converter()
converterMap+=("fc_feed_rs_components_fr"->dummyobj_fc_feed_rs_components_fr)
class fc_feed_kruse_oems_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_kruse_oems(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_kruse_oems = new fc_feed_kruse_oems_converter()
converterMap+=("fc_feed_kruse_oems"->dummyobj_fc_feed_kruse_oems)
class fc_feed_rs_components_emea_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_emea(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_emea = new fc_feed_rs_components_emea_converter()
converterMap+=("fc_feed_rs_components_emea"->dummyobj_fc_feed_rs_components_emea)
class fc_feed_electronicount_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_electronicount(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_electronicount = new fc_feed_electronicount_converter()
converterMap+=("fc_feed_electronicount"->dummyobj_fc_feed_electronicount)
class fc_feed_rs_components_se_mft_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_se_mft_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_se_mft_bnl = new fc_feed_rs_components_se_mft_bnl_converter()
converterMap+=("fc_feed_rs_components_se_mft_bnl"->dummyobj_fc_feed_rs_components_se_mft_bnl)
class fc_feed_nacsemi_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_nacsemi_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_nacsemi_bnl = new fc_feed_nacsemi_bnl_converter()
converterMap+=("fc_feed_nacsemi_bnl"->dummyobj_fc_feed_nacsemi_bnl)
class fc_feed_rspro_ru_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rspro_ru(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rspro_ru = new fc_feed_rspro_ru_converter()
converterMap+=("fc_feed_rspro_ru"->dummyobj_fc_feed_rspro_ru)
class fc_feed_farnell_de_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_farnell_de(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_farnell_de = new fc_feed_farnell_de_converter()
converterMap+=("fc_feed_farnell_de"->dummyobj_fc_feed_farnell_de)
class fc_feed_shikues_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_shikues(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_shikues = new fc_feed_shikues_converter()
converterMap+=("fc_feed_shikues"->dummyobj_fc_feed_shikues)
class fc_feed_ibuyxs_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_ibuyxs_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_ibuyxs_bnl = new fc_feed_ibuyxs_bnl_converter()
converterMap+=("fc_feed_ibuyxs_bnl"->dummyobj_fc_feed_ibuyxs_bnl)
class fc_feed_distrelec_ee_findchips_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_distrelec_ee_findchips(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_distrelec_ee_findchips = new fc_feed_distrelec_ee_findchips_converter()
converterMap+=("fc_feed_distrelec_ee_findchips"->dummyobj_fc_feed_distrelec_ee_findchips)
class fc_feed_sielectronics_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_sielectronics(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_sielectronics = new fc_feed_sielectronics_converter()
converterMap+=("fc_feed_sielectronics"->dummyobj_fc_feed_sielectronics)
class fc_feed_stockers_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_stockers(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_stockers = new fc_feed_stockers_converter()
converterMap+=("fc_feed_stockers"->dummyobj_fc_feed_stockers)
class fc_feed_j2_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_j2(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_j2 = new fc_feed_j2_converter()
converterMap+=("fc_feed_j2"->dummyobj_fc_feed_j2)
class fc_feed_sense_elec_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_sense_elec(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_sense_elec = new fc_feed_sense_elec_converter()
converterMap+=("fc_feed_sense_elec"->dummyobj_fc_feed_sense_elec)
class fc_feed_bdrtech_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_bdrtech(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_bdrtech = new fc_feed_bdrtech_converter()
converterMap+=("fc_feed_bdrtech"->dummyobj_fc_feed_bdrtech)
class fc_feed_superman_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_superman(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_superman = new fc_feed_superman_converter()
converterMap+=("fc_feed_superman"->dummyobj_fc_feed_superman)
class fc_feed_lcsc_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_lcsc(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_lcsc = new fc_feed_lcsc_converter()
converterMap+=("fc_feed_lcsc"->dummyobj_fc_feed_lcsc)
class fc_feed_farnell_si_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_farnell_si(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_farnell_si = new fc_feed_farnell_si_converter()
converterMap+=("fc_feed_farnell_si"->dummyobj_fc_feed_farnell_si)
class fc_feed_rs_global_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_global(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_global = new fc_feed_rs_global_converter()
converterMap+=("fc_feed_rs_global"->dummyobj_fc_feed_rs_global)
class fc_feed_peigenesis_us_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_peigenesis_us(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_peigenesis_us = new fc_feed_peigenesis_us_converter()
converterMap+=("fc_feed_peigenesis_us"->dummyobj_fc_feed_peigenesis_us)
class fc_feed_b2b_comsit_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_b2b_comsit(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_b2b_comsit = new fc_feed_b2b_comsit_converter()
converterMap+=("fc_feed_b2b_comsit"->dummyobj_fc_feed_b2b_comsit)
class fc_feed_hengfeng_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_hengfeng(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_hengfeng = new fc_feed_hengfeng_converter()
converterMap+=("fc_feed_hengfeng"->dummyobj_fc_feed_hengfeng)
class fc_feed_nexgen_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_nexgen(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_nexgen = new fc_feed_nexgen_converter()
converterMap+=("fc_feed_nexgen"->dummyobj_fc_feed_nexgen)
class fc_feed_rs_components_no_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_no(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_no = new fc_feed_rs_components_no_converter()
converterMap+=("fc_feed_rs_components_no"->dummyobj_fc_feed_rs_components_no)
class fc_feed_chip_digger_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_chip_digger(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_chip_digger = new fc_feed_chip_digger_converter()
converterMap+=("fc_feed_chip_digger"->dummyobj_fc_feed_chip_digger)
class fc_feed_rs_components_it_mft_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_it_mft_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_it_mft_bnl = new fc_feed_rs_components_it_mft_bnl_converter()
converterMap+=("fc_feed_rs_components_it_mft_bnl"->dummyobj_fc_feed_rs_components_it_mft_bnl)
class fc_feed_digikey_bnl_lookup_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_digikey_bnl_lookup(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_digikey_bnl_lookup = new fc_feed_digikey_bnl_lookup_converter()
converterMap+=("fc_feed_digikey_bnl_lookup"->dummyobj_fc_feed_digikey_bnl_lookup)
class fc_feed_plcdirect_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_plcdirect(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_plcdirect = new fc_feed_plcdirect_converter()
converterMap+=("fc_feed_plcdirect"->dummyobj_fc_feed_plcdirect)
class fc_feed_master_oems_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_master_oems(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_master_oems = new fc_feed_master_oems_converter()
converterMap+=("fc_feed_master_oems"->dummyobj_fc_feed_master_oems)
class fc_feed_chipchip_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_chipchip(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_chipchip = new fc_feed_chipchip_converter()
converterMap+=("fc_feed_chipchip"->dummyobj_fc_feed_chipchip)
class fc_feed_farnell_lt_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_farnell_lt(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_farnell_lt = new fc_feed_farnell_lt_converter()
converterMap+=("fc_feed_farnell_lt"->dummyobj_fc_feed_farnell_lt)
class fc_feed_hongte_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_hongte(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_hongte = new fc_feed_hongte_converter()
converterMap+=("fc_feed_hongte"->dummyobj_fc_feed_hongte)
class fc_feed_verical_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_verical_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_verical_bnl = new fc_feed_verical_bnl_converter()
converterMap+=("fc_feed_verical_bnl"->dummyobj_fc_feed_verical_bnl)
class fc_feed_farnell_bg_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_farnell_bg(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_farnell_bg = new fc_feed_farnell_bg_converter()
converterMap+=("fc_feed_farnell_bg"->dummyobj_fc_feed_farnell_bg)
class fc_feed_rspro_sk_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rspro_sk(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rspro_sk = new fc_feed_rspro_sk_converter()
converterMap+=("fc_feed_rspro_sk"->dummyobj_fc_feed_rspro_sk)
class fc_feed_walker_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_walker(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_walker = new fc_feed_walker_converter()
converterMap+=("fc_feed_walker"->dummyobj_fc_feed_walker)
class fc_feed_chip1stop_jpy_oems_2_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_chip1stop_jpy_oems_2(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_chip1stop_jpy_oems_2 = new fc_feed_chip1stop_jpy_oems_2_converter()
converterMap+=("fc_feed_chip1stop_jpy_oems_2"->dummyobj_fc_feed_chip1stop_jpy_oems_2)
class fc_feed_electronictreasures_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_electronictreasures(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_electronictreasures = new fc_feed_electronictreasures_converter()
converterMap+=("fc_feed_electronictreasures"->dummyobj_fc_feed_electronictreasures)
class fc_feed_farnell_ie_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_farnell_ie(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_farnell_ie = new fc_feed_farnell_ie_converter()
converterMap+=("fc_feed_farnell_ie"->dummyobj_fc_feed_farnell_ie)
class fc_feed_distrelec_lt_oemstrade_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_distrelec_lt_oemstrade(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_distrelec_lt_oemstrade = new fc_feed_distrelec_lt_oemstrade_converter()
converterMap+=("fc_feed_distrelec_lt_oemstrade"->dummyobj_fc_feed_distrelec_lt_oemstrade)
class fc_feed_bisco_bnl_2022_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_bisco_bnl_2022(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_bisco_bnl_2022 = new fc_feed_bisco_bnl_2022_converter()
converterMap+=("fc_feed_bisco_bnl_2022"->dummyobj_fc_feed_bisco_bnl_2022)
class fc_feed_farnell_at_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_farnell_at(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_farnell_at = new fc_feed_farnell_at_converter()
converterMap+=("fc_feed_farnell_at"->dummyobj_fc_feed_farnell_at)
class fc_feed_rspro_ro_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rspro_ro(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rspro_ro = new fc_feed_rspro_ro_converter()
converterMap+=("fc_feed_rspro_ro"->dummyobj_fc_feed_rspro_ro)
class fc_feed_digikey_de_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_digikey_de_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_digikey_de_bnl = new fc_feed_digikey_de_bnl_converter()
converterMap+=("fc_feed_digikey_de_bnl"->dummyobj_fc_feed_digikey_de_bnl)
class fc_feed_vigor_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_vigor(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_vigor = new fc_feed_vigor_converter()
converterMap+=("fc_feed_vigor"->dummyobj_fc_feed_vigor)
class fc_feed_microchip_usa_oems_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_microchip_usa_oems(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_microchip_usa_oems = new fc_feed_microchip_usa_oems_converter()
converterMap+=("fc_feed_microchip_usa_oems"->dummyobj_fc_feed_microchip_usa_oems)
class fc_feed_farnell_nl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_farnell_nl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_farnell_nl = new fc_feed_farnell_nl_converter()
converterMap+=("fc_feed_farnell_nl"->dummyobj_fc_feed_farnell_nl)
class fc_feed_rs_components_no_mft_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_no_mft_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_no_mft_bnl = new fc_feed_rs_components_no_mft_bnl_converter()
converterMap+=("fc_feed_rs_components_no_mft_bnl"->dummyobj_fc_feed_rs_components_no_mft_bnl)
class fc_feed_rs_components_dk_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_dk(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_dk = new fc_feed_rs_components_dk_converter()
converterMap+=("fc_feed_rs_components_dk"->dummyobj_fc_feed_rs_components_dk)
class fc_feed_yicintl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_yicintl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_yicintl = new fc_feed_yicintl_converter()
converterMap+=("fc_feed_yicintl"->dummyobj_fc_feed_yicintl)
class fc_feed_newadvantage_oems_us_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_newadvantage_oems_us(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_newadvantage_oems_us = new fc_feed_newadvantage_oems_us_converter()
converterMap+=("fc_feed_newadvantage_oems_us"->dummyobj_fc_feed_newadvantage_oems_us)
class fc_feed_lcomus_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_lcomus(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_lcomus = new fc_feed_lcomus_converter()
converterMap+=("fc_feed_lcomus"->dummyobj_fc_feed_lcomus)
class fc_feed_bisco_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_bisco(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_bisco = new fc_feed_bisco_converter()
converterMap+=("fc_feed_bisco"->dummyobj_fc_feed_bisco)
class fc_feed_ysyelec_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_ysyelec(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_ysyelec = new fc_feed_ysyelec_converter()
converterMap+=("fc_feed_ysyelec"->dummyobj_fc_feed_ysyelec)
class fc_feed_hk_keep_booming_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_hk_keep_booming(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_hk_keep_booming = new fc_feed_hk_keep_booming_converter()
converterMap+=("fc_feed_hk_keep_booming"->dummyobj_fc_feed_hk_keep_booming)
class fc_feed_distrelec_fr_findchips_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_distrelec_fr_findchips(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_distrelec_fr_findchips = new fc_feed_distrelec_fr_findchips_converter()
converterMap+=("fc_feed_distrelec_fr_findchips"->dummyobj_fc_feed_distrelec_fr_findchips)
class fc_feed_rochester_kr_ti_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rochester_kr_ti_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rochester_kr_ti_bnl = new fc_feed_rochester_kr_ti_bnl_converter()
converterMap+=("fc_feed_rochester_kr_ti_bnl"->dummyobj_fc_feed_rochester_kr_ti_bnl)
class fc_feed_fairview_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_fairview(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_fairview = new fc_feed_fairview_converter()
converterMap+=("fc_feed_fairview"->dummyobj_fc_feed_fairview)
class fc_feed_centum_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_centum(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_centum = new fc_feed_centum_converter()
converterMap+=("fc_feed_centum"->dummyobj_fc_feed_centum)
class fc_feed_ibuyxs_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_ibuyxs(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_ibuyxs = new fc_feed_ibuyxs_converter()
converterMap+=("fc_feed_ibuyxs"->dummyobj_fc_feed_ibuyxs)
class fc_feed_ichunt_fc_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_ichunt_fc(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_ichunt_fc = new fc_feed_ichunt_fc_converter()
converterMap+=("fc_feed_ichunt_fc"->dummyobj_fc_feed_ichunt_fc)
class fc_feed_fdh_electronics_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_fdh_electronics(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_fdh_electronics = new fc_feed_fdh_electronics_converter()
converterMap+=("fc_feed_fdh_electronics"->dummyobj_fc_feed_fdh_electronics)
class fc_feed_rs_europe_bnl_it_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_europe_bnl_it(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_europe_bnl_it = new fc_feed_rs_europe_bnl_it_converter()
converterMap+=("fc_feed_rs_europe_bnl_it"->dummyobj_fc_feed_rs_europe_bnl_it)
class fc_feed_peigenesis_005_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_peigenesis_005(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_peigenesis_005 = new fc_feed_peigenesis_005_converter()
converterMap+=("fc_feed_peigenesis_005"->dummyobj_fc_feed_peigenesis_005)
class fc_feed_rs_components_es_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_es(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_es = new fc_feed_rs_components_es_converter()
converterMap+=("fc_feed_rs_components_es"->dummyobj_fc_feed_rs_components_es)
class fc_feed_tme_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_tme(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_tme = new fc_feed_tme_converter()
converterMap+=("fc_feed_tme"->dummyobj_fc_feed_tme)
class fc_feed_lcsc_oems_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_lcsc_oems(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_lcsc_oems = new fc_feed_lcsc_oems_converter()
converterMap+=("fc_feed_lcsc_oems"->dummyobj_fc_feed_lcsc_oems)
class fc_feed_pasternack_us_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_pasternack_us(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_pasternack_us = new fc_feed_pasternack_us_converter()
converterMap+=("fc_feed_pasternack_us"->dummyobj_fc_feed_pasternack_us)
class fc_feed_element14_sg_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_element14_sg(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_element14_sg = new fc_feed_element14_sg_converter()
converterMap+=("fc_feed_element14_sg"->dummyobj_fc_feed_element14_sg)
class fc_feed_rs_components_es_mft_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_es_mft_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_es_mft_bnl = new fc_feed_rs_components_es_mft_bnl_converter()
converterMap+=("fc_feed_rs_components_es_mft_bnl"->dummyobj_fc_feed_rs_components_es_mft_bnl)
class fc_feed_distrelec_ee_oemstrade_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_distrelec_ee_oemstrade(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_distrelec_ee_oemstrade = new fc_feed_distrelec_ee_oemstrade_converter()
converterMap+=("fc_feed_distrelec_ee_oemstrade"->dummyobj_fc_feed_distrelec_ee_oemstrade)
class fc_feed_ti_bnl_epd_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_ti_bnl_epd(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_ti_bnl_epd = new fc_feed_ti_bnl_epd_converter()
converterMap+=("fc_feed_ti_bnl_epd"->dummyobj_fc_feed_ti_bnl_epd)
class fc_feed_rs_components_de_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_de(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_de = new fc_feed_rs_components_de_converter()
converterMap+=("fc_feed_rs_components_de"->dummyobj_fc_feed_rs_components_de)
class fc_feed_digikey_cn_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_digikey_cn_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_digikey_cn_bnl = new fc_feed_digikey_cn_bnl_converter()
converterMap+=("fc_feed_digikey_cn_bnl"->dummyobj_fc_feed_digikey_cn_bnl)
class fc_feed_digikey_bnl_emea_test_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_digikey_bnl_emea_test(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_digikey_bnl_emea_test = new fc_feed_digikey_bnl_emea_test_converter()
converterMap+=("fc_feed_digikey_bnl_emea_test"->dummyobj_fc_feed_digikey_bnl_emea_test)
class fc_feed_rochester_cn_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rochester_cn(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rochester_cn = new fc_feed_rochester_cn_converter()
converterMap+=("fc_feed_rochester_cn"->dummyobj_fc_feed_rochester_cn)
class fc_feed_k1tech_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_k1tech(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_k1tech = new fc_feed_k1tech_converter()
converterMap+=("fc_feed_k1tech"->dummyobj_fc_feed_k1tech)
class fc_feed_distrelec_lt_findchips_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_distrelec_lt_findchips(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_distrelec_lt_findchips = new fc_feed_distrelec_lt_findchips_converter()
converterMap+=("fc_feed_distrelec_lt_findchips"->dummyobj_fc_feed_distrelec_lt_findchips)
class fc_feed_farnell_hu_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_farnell_hu(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_farnell_hu = new fc_feed_farnell_hu_converter()
converterMap+=("fc_feed_farnell_hu"->dummyobj_fc_feed_farnell_hu)
class fc_feed_ti_bnl_epd2_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_ti_bnl_epd2(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_ti_bnl_epd2 = new fc_feed_ti_bnl_epd2_converter()
converterMap+=("fc_feed_ti_bnl_epd2"->dummyobj_fc_feed_ti_bnl_epd2)
class fc_feed_btc_oemstrade_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_btc_oemstrade(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_btc_oemstrade = new fc_feed_btc_oemstrade_converter()
converterMap+=("fc_feed_btc_oemstrade"->dummyobj_fc_feed_btc_oemstrade)
class fc_feed_breizelec_fc_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_breizelec_fc(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_breizelec_fc = new fc_feed_breizelec_fc_converter()
converterMap+=("fc_feed_breizelec_fc"->dummyobj_fc_feed_breizelec_fc)
class fc_feed_farnell_fcpro_apac_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_farnell_fcpro_apac(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_farnell_fcpro_apac = new fc_feed_farnell_fcpro_apac_converter()
converterMap+=("fc_feed_farnell_fcpro_apac"->dummyobj_fc_feed_farnell_fcpro_apac)
class fc_feed_icc_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_icc(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_icc = new fc_feed_icc_converter()
converterMap+=("fc_feed_icc"->dummyobj_fc_feed_icc)
class fc_feed_waytek_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_waytek_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_waytek_bnl = new fc_feed_waytek_bnl_converter()
converterMap+=("fc_feed_waytek_bnl"->dummyobj_fc_feed_waytek_bnl)
class fc_feed_farnell_rohde_bnl_de_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_farnell_rohde_bnl_de(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_farnell_rohde_bnl_de = new fc_feed_farnell_rohde_bnl_de_converter()
converterMap+=("fc_feed_farnell_rohde_bnl_de"->dummyobj_fc_feed_farnell_rohde_bnl_de)
class fc_feed_ikeyparts_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_ikeyparts(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_ikeyparts = new fc_feed_ikeyparts_converter()
converterMap+=("fc_feed_ikeyparts"->dummyobj_fc_feed_ikeyparts)
class fc_feed_link_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_link(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_link = new fc_feed_link_converter()
converterMap+=("fc_feed_link"->dummyobj_fc_feed_link)
class fc_feed_comsit_fc_mx_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_comsit_fc_mx(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_comsit_fc_mx = new fc_feed_comsit_fc_mx_converter()
converterMap+=("fc_feed_comsit_fc_mx"->dummyobj_fc_feed_comsit_fc_mx)
class fc_feed_jingyaorun_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_jingyaorun(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_jingyaorun = new fc_feed_jingyaorun_converter()
converterMap+=("fc_feed_jingyaorun"->dummyobj_fc_feed_jingyaorun)
class fc_feed_rs_components_pt_mft_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_pt_mft_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_pt_mft_bnl = new fc_feed_rs_components_pt_mft_bnl_converter()
converterMap+=("fc_feed_rs_components_pt_mft_bnl"->dummyobj_fc_feed_rs_components_pt_mft_bnl)
class fc_feed_renesas_digikey_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_renesas_digikey(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_renesas_digikey = new fc_feed_renesas_digikey_converter()
converterMap+=("fc_feed_renesas_digikey"->dummyobj_fc_feed_renesas_digikey)
class fc_feed_lcom_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_lcom(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_lcom = new fc_feed_lcom_converter()
converterMap+=("fc_feed_lcom"->dummyobj_fc_feed_lcom)
class fc_feed_shirakaba_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_shirakaba(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_shirakaba = new fc_feed_shirakaba_converter()
converterMap+=("fc_feed_shirakaba"->dummyobj_fc_feed_shirakaba)
class fc_feed_rspro_pt_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rspro_pt(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rspro_pt = new fc_feed_rspro_pt_converter()
converterMap+=("fc_feed_rspro_pt"->dummyobj_fc_feed_rspro_pt)
class fc_feed_comsit_oems_fr_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_comsit_oems_fr(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_comsit_oems_fr = new fc_feed_comsit_oems_fr_converter()
converterMap+=("fc_feed_comsit_oems_fr"->dummyobj_fc_feed_comsit_oems_fr)
class fc_feed_ayelectronics_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_ayelectronics(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_ayelectronics = new fc_feed_ayelectronics_converter()
converterMap+=("fc_feed_ayelectronics"->dummyobj_fc_feed_ayelectronics)
class fc_feed_rochester_ti_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rochester_ti_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rochester_ti_bnl = new fc_feed_rochester_ti_bnl_converter()
converterMap+=("fc_feed_rochester_ti_bnl"->dummyobj_fc_feed_rochester_ti_bnl)
class fc_feed_dicchip_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_dicchip(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_dicchip = new fc_feed_dicchip_converter()
converterMap+=("fc_feed_dicchip"->dummyobj_fc_feed_dicchip)
class fc_feed_ti_bnl_hval_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_ti_bnl_hval(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_ti_bnl_hval = new fc_feed_ti_bnl_hval_converter()
converterMap+=("fc_feed_ti_bnl_hval"->dummyobj_fc_feed_ti_bnl_hval)
class fc_feed_rs_components_dk_mft_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_dk_mft_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_dk_mft_bnl = new fc_feed_rs_components_dk_mft_bnl_converter()
converterMap+=("fc_feed_rs_components_dk_mft_bnl"->dummyobj_fc_feed_rs_components_dk_mft_bnl)
class fc_feed_rutronik_cn_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rutronik_cn(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rutronik_cn = new fc_feed_rutronik_cn_converter()
converterMap+=("fc_feed_rutronik_cn"->dummyobj_fc_feed_rutronik_cn)
class fc_feed_rgelek_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rgelek(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rgelek = new fc_feed_rgelek_converter()
converterMap+=("fc_feed_rgelek"->dummyobj_fc_feed_rgelek)
class fc_feed_peigenesis_001_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_peigenesis_001_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_peigenesis_001_bnl = new fc_feed_peigenesis_001_bnl_converter()
converterMap+=("fc_feed_peigenesis_001_bnl"->dummyobj_fc_feed_peigenesis_001_bnl)
class fc_feed_mouser_bnl_apac_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_mouser_bnl_apac(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_mouser_bnl_apac = new fc_feed_mouser_bnl_apac_converter()
converterMap+=("fc_feed_mouser_bnl_apac"->dummyobj_fc_feed_mouser_bnl_apac)
class fc_feed_future_china_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_future_china(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_future_china = new fc_feed_future_china_converter()
converterMap+=("fc_feed_future_china"->dummyobj_fc_feed_future_china)
class fc_feed_ibs_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_ibs(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_ibs = new fc_feed_ibs_converter()
converterMap+=("fc_feed_ibs"->dummyobj_fc_feed_ibs)
class fc_feed_global_sourcing_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_global_sourcing(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_global_sourcing = new fc_feed_global_sourcing_converter()
converterMap+=("fc_feed_global_sourcing"->dummyobj_fc_feed_global_sourcing)
class fc_feed_newark_ca_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_newark_ca(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_newark_ca = new fc_feed_newark_ca_converter()
converterMap+=("fc_feed_newark_ca"->dummyobj_fc_feed_newark_ca)
class fc_feed_rs_components_nl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_nl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_nl = new fc_feed_rs_components_nl_converter()
converterMap+=("fc_feed_rs_components_nl"->dummyobj_fc_feed_rs_components_nl)
class fc_feed_adafruit_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_adafruit(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_adafruit = new fc_feed_adafruit_converter()
converterMap+=("fc_feed_adafruit"->dummyobj_fc_feed_adafruit)
class fc_feed_drex_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_drex(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_drex = new fc_feed_drex_converter()
converterMap+=("fc_feed_drex"->dummyobj_fc_feed_drex)
class fc_feed_nacsemi_oems_us_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_nacsemi_oems_us(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_nacsemi_oems_us = new fc_feed_nacsemi_oems_us_converter()
converterMap+=("fc_feed_nacsemi_oems_us"->dummyobj_fc_feed_nacsemi_oems_us)
class fc_feed_hkcinty_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_hkcinty(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_hkcinty = new fc_feed_hkcinty_converter()
converterMap+=("fc_feed_hkcinty"->dummyobj_fc_feed_hkcinty)
class fc_feed_chip1cloud_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_chip1cloud(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_chip1cloud = new fc_feed_chip1cloud_converter()
converterMap+=("fc_feed_chip1cloud"->dummyobj_fc_feed_chip1cloud)
class fc_feed_esino_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_esino(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_esino = new fc_feed_esino_converter()
converterMap+=("fc_feed_esino"->dummyobj_fc_feed_esino)
class fc_feed_detailtech_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_detailtech(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_detailtech = new fc_feed_detailtech_converter()
converterMap+=("fc_feed_detailtech"->dummyobj_fc_feed_detailtech)
class fc_feed_distrelec_it_oemstrade_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_distrelec_it_oemstrade(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_distrelec_it_oemstrade = new fc_feed_distrelec_it_oemstrade_converter()
converterMap+=("fc_feed_distrelec_it_oemstrade"->dummyobj_fc_feed_distrelec_it_oemstrade)
class fc_feed_tme_bnl_it_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_tme_bnl_it(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_tme_bnl_it = new fc_feed_tme_bnl_it_converter()
converterMap+=("fc_feed_tme_bnl_it"->dummyobj_fc_feed_tme_bnl_it)
class fc_feed_btc_findchips_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_btc_findchips(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_btc_findchips = new fc_feed_btc_findchips_converter()
converterMap+=("fc_feed_btc_findchips"->dummyobj_fc_feed_btc_findchips)
class fc_feed_oneyac_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_oneyac(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_oneyac = new fc_feed_oneyac_converter()
converterMap+=("fc_feed_oneyac"->dummyobj_fc_feed_oneyac)
class fc_feed_olc_bnl_kilovac_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_olc_bnl_kilovac(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_olc_bnl_kilovac = new fc_feed_olc_bnl_kilovac_converter()
converterMap+=("fc_feed_olc_bnl_kilovac"->dummyobj_fc_feed_olc_bnl_kilovac)
class fc_feed_farnell_jp_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_farnell_jp(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_farnell_jp = new fc_feed_farnell_jp_converter()
converterMap+=("fc_feed_farnell_jp"->dummyobj_fc_feed_farnell_jp)
class fc_feed_superchip_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_superchip(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_superchip = new fc_feed_superchip_converter()
converterMap+=("fc_feed_superchip"->dummyobj_fc_feed_superchip)
class fc_feed_cgelec_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_cgelec(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_cgelec = new fc_feed_cgelec_converter()
converterMap+=("fc_feed_cgelec"->dummyobj_fc_feed_cgelec)
class fc_feed_rs_components_my_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_my(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_my = new fc_feed_rs_components_my_converter()
converterMap+=("fc_feed_rs_components_my"->dummyobj_fc_feed_rs_components_my)
class fc_feed_grandpower_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_grandpower(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_grandpower = new fc_feed_grandpower_converter()
converterMap+=("fc_feed_grandpower"->dummyobj_fc_feed_grandpower)
class fc_feed_element14_au_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_element14_au(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_element14_au = new fc_feed_element14_au_converter()
converterMap+=("fc_feed_element14_au"->dummyobj_fc_feed_element14_au)
class fc_feed_rs_components_tw_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_tw(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_tw = new fc_feed_rs_components_tw_converter()
converterMap+=("fc_feed_rs_components_tw"->dummyobj_fc_feed_rs_components_tw)
class fc_feed_comsit_fc_cn_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_comsit_fc_cn(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_comsit_fc_cn = new fc_feed_comsit_fc_cn_converter()
converterMap+=("fc_feed_comsit_fc_cn"->dummyobj_fc_feed_comsit_fc_cn)
class fc_feed_shengyu_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_shengyu(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_shengyu = new fc_feed_shengyu_converter()
converterMap+=("fc_feed_shengyu"->dummyobj_fc_feed_shengyu)
class fc_feed_opulent_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_opulent(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_opulent = new fc_feed_opulent_converter()
converterMap+=("fc_feed_opulent"->dummyobj_fc_feed_opulent)
class fc_feed_rs_components_at_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_at(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_at = new fc_feed_rs_components_at_converter()
converterMap+=("fc_feed_rs_components_at"->dummyobj_fc_feed_rs_components_at)
class fc_feed_ezkey_oems_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_ezkey_oems(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_ezkey_oems = new fc_feed_ezkey_oems_converter()
converterMap+=("fc_feed_ezkey_oems"->dummyobj_fc_feed_ezkey_oems)
class fc_feed_comsit_fc_br_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_comsit_fc_br(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_comsit_fc_br = new fc_feed_comsit_fc_br_converter()
converterMap+=("fc_feed_comsit_fc_br"->dummyobj_fc_feed_comsit_fc_br)
class fc_feed_rs_components_nl_mft_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_nl_mft_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_nl_mft_bnl = new fc_feed_rs_components_nl_mft_bnl_converter()
converterMap+=("fc_feed_rs_components_nl_mft_bnl"->dummyobj_fc_feed_rs_components_nl_mft_bnl)
class fc_feed_avnet_europe_abacus_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_avnet_europe_abacus(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_avnet_europe_abacus = new fc_feed_avnet_europe_abacus_converter()
converterMap+=("fc_feed_avnet_europe_abacus"->dummyobj_fc_feed_avnet_europe_abacus)
class fc_feed_cuidevices_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_cuidevices(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_cuidevices = new fc_feed_cuidevices_converter()
converterMap+=("fc_feed_cuidevices"->dummyobj_fc_feed_cuidevices)
class fc_feed_chip1stop_usd_oems_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_chip1stop_usd_oems(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_chip1stop_usd_oems = new fc_feed_chip1stop_usd_oems_converter()
converterMap+=("fc_feed_chip1stop_usd_oems"->dummyobj_fc_feed_chip1stop_usd_oems)
class fc_feed_burklin_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_burklin(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_burklin = new fc_feed_burklin_converter()
converterMap+=("fc_feed_burklin"->dummyobj_fc_feed_burklin)
class fc_feed_sehot_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_sehot(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_sehot = new fc_feed_sehot_converter()
converterMap+=("fc_feed_sehot"->dummyobj_fc_feed_sehot)
class fc_feed_tpsglobal_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_tpsglobal(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_tpsglobal = new fc_feed_tpsglobal_converter()
converterMap+=("fc_feed_tpsglobal"->dummyobj_fc_feed_tpsglobal)
class fc_feed_cicmaster_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_cicmaster(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_cicmaster = new fc_feed_cicmaster_converter()
converterMap+=("fc_feed_cicmaster"->dummyobj_fc_feed_cicmaster)
class fc_feed_digikey_kr_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_digikey_kr(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_digikey_kr = new fc_feed_digikey_kr_converter()
converterMap+=("fc_feed_digikey_kr"->dummyobj_fc_feed_digikey_kr)
class fc_feed_dasenic_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_dasenic(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_dasenic = new fc_feed_dasenic_converter()
converterMap+=("fc_feed_dasenic"->dummyobj_fc_feed_dasenic)
class fc_feed_americanmicro_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_americanmicro(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_americanmicro = new fc_feed_americanmicro_converter()
converterMap+=("fc_feed_americanmicro"->dummyobj_fc_feed_americanmicro)
class fc_feed_farnell_rp_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_farnell_rp_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_farnell_rp_bnl = new fc_feed_farnell_rp_bnl_converter()
converterMap+=("fc_feed_farnell_rp_bnl"->dummyobj_fc_feed_farnell_rp_bnl)
class fc_feed_newadvantage_fc_us_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_newadvantage_fc_us(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_newadvantage_fc_us = new fc_feed_newadvantage_fc_us_converter()
converterMap+=("fc_feed_newadvantage_fc_us"->dummyobj_fc_feed_newadvantage_fc_us)
class fc_feed_frontview_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_frontview(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_frontview = new fc_feed_frontview_converter()
converterMap+=("fc_feed_frontview"->dummyobj_fc_feed_frontview)
class fc_feed_tme_lookup_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_tme_lookup_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_tme_lookup_bnl = new fc_feed_tme_lookup_bnl_converter()
converterMap+=("fc_feed_tme_lookup_bnl"->dummyobj_fc_feed_tme_lookup_bnl)
class fc_feed_compelec_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_compelec(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_compelec = new fc_feed_compelec_converter()
converterMap+=("fc_feed_compelec"->dummyobj_fc_feed_compelec)
class fc_feed_saiaosi_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_saiaosi(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_saiaosi = new fc_feed_saiaosi_converter()
converterMap+=("fc_feed_saiaosi"->dummyobj_fc_feed_saiaosi)
class fc_feed_rs_components_us_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_us(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_us = new fc_feed_rs_components_us_converter()
converterMap+=("fc_feed_rs_components_us"->dummyobj_fc_feed_rs_components_us)
class fc_feed_lantana_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_lantana(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_lantana = new fc_feed_lantana_converter()
converterMap+=("fc_feed_lantana"->dummyobj_fc_feed_lantana)
class fc_feed_rspro_be_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rspro_be(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rspro_be = new fc_feed_rspro_be_converter()
converterMap+=("fc_feed_rspro_be"->dummyobj_fc_feed_rspro_be)
class fc_feed_appelec_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_appelec(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_appelec = new fc_feed_appelec_converter()
converterMap+=("fc_feed_appelec"->dummyobj_fc_feed_appelec)
class fc_feed_farnell_it_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_farnell_it(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_farnell_it = new fc_feed_farnell_it_converter()
converterMap+=("fc_feed_farnell_it"->dummyobj_fc_feed_farnell_it)
class fc_feed_icc_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_icc_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_icc_bnl = new fc_feed_icc_bnl_converter()
converterMap+=("fc_feed_icc_bnl"->dummyobj_fc_feed_icc_bnl)
class fc_feed_verical_global_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_verical_global(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_verical_global = new fc_feed_verical_global_converter()
converterMap+=("fc_feed_verical_global"->dummyobj_fc_feed_verical_global)
class fc_feed_rs_components_molex_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_molex_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_molex_bnl = new fc_feed_rs_components_molex_bnl_converter()
converterMap+=("fc_feed_rs_components_molex_bnl"->dummyobj_fc_feed_rs_components_molex_bnl)
class fc_feed_findfpga_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_findfpga(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_findfpga = new fc_feed_findfpga_converter()
converterMap+=("fc_feed_findfpga"->dummyobj_fc_feed_findfpga)
class fc_feed_arrow_ads_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_arrow_ads(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_arrow_ads = new fc_feed_arrow_ads_converter()
converterMap+=("fc_feed_arrow_ads"->dummyobj_fc_feed_arrow_ads)
class fc_feed_rs_americas_bnl_siemens_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_americas_bnl_siemens(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_americas_bnl_siemens = new fc_feed_rs_americas_bnl_siemens_converter()
converterMap+=("fc_feed_rs_americas_bnl_siemens"->dummyobj_fc_feed_rs_americas_bnl_siemens)
class fc_feed_powell_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_powell(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_powell = new fc_feed_powell_converter()
converterMap+=("fc_feed_powell"->dummyobj_fc_feed_powell)
class fc_feed_rs_components_hu_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_hu(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_hu = new fc_feed_rs_components_hu_converter()
converterMap+=("fc_feed_rs_components_hu"->dummyobj_fc_feed_rs_components_hu)
class fc_feed_digikey_de_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_digikey_de(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_digikey_de = new fc_feed_digikey_de_converter()
converterMap+=("fc_feed_digikey_de"->dummyobj_fc_feed_digikey_de)
class fc_feed_futuretech_fc_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_futuretech_fc(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_futuretech_fc = new fc_feed_futuretech_fc_converter()
converterMap+=("fc_feed_futuretech_fc"->dummyobj_fc_feed_futuretech_fc)
class fc_feed_distrelec_be_oemstrade_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_distrelec_be_oemstrade(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_distrelec_be_oemstrade = new fc_feed_distrelec_be_oemstrade_converter()
converterMap+=("fc_feed_distrelec_be_oemstrade"->dummyobj_fc_feed_distrelec_be_oemstrade)
class fc_feed_coilcraft_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_coilcraft(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_coilcraft = new fc_feed_coilcraft_converter()
converterMap+=("fc_feed_coilcraft"->dummyobj_fc_feed_coilcraft)
class fc_feed_rs_components_kr_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_kr(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_kr = new fc_feed_rs_components_kr_converter()
converterMap+=("fc_feed_rs_components_kr"->dummyobj_fc_feed_rs_components_kr)
class fc_feed_pui_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_pui(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_pui = new fc_feed_pui_converter()
converterMap+=("fc_feed_pui"->dummyobj_fc_feed_pui)
class fc_feed_southchip_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_southchip(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_southchip = new fc_feed_southchip_converter()
converterMap+=("fc_feed_southchip"->dummyobj_fc_feed_southchip)
class fc_feed_megastar_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_megastar(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_megastar = new fc_feed_megastar_converter()
converterMap+=("fc_feed_megastar"->dummyobj_fc_feed_megastar)
class fc_feed_ameya_fc_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_ameya_fc(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_ameya_fc = new fc_feed_ameya_fc_converter()
converterMap+=("fc_feed_ameya_fc"->dummyobj_fc_feed_ameya_fc)
class fc_feed_bestcomp_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_bestcomp(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_bestcomp = new fc_feed_bestcomp_converter()
converterMap+=("fc_feed_bestcomp"->dummyobj_fc_feed_bestcomp)
class fc_feed_heilind_world_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_heilind_world(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_heilind_world = new fc_feed_heilind_world_converter()
converterMap+=("fc_feed_heilind_world"->dummyobj_fc_feed_heilind_world)
class fc_feed_tme_bnl_si_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_tme_bnl_si(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_tme_bnl_si = new fc_feed_tme_bnl_si_converter()
converterMap+=("fc_feed_tme_bnl_si"->dummyobj_fc_feed_tme_bnl_si)
class fc_feed_perfectparts_oems_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_perfectparts_oems(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_perfectparts_oems = new fc_feed_perfectparts_oems_converter()
converterMap+=("fc_feed_perfectparts_oems"->dummyobj_fc_feed_perfectparts_oems)
class fc_feed_chuanghan_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_chuanghan(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_chuanghan = new fc_feed_chuanghan_converter()
converterMap+=("fc_feed_chuanghan"->dummyobj_fc_feed_chuanghan)
class fc_feed_testequity_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_testequity(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_testequity = new fc_feed_testequity_converter()
converterMap+=("fc_feed_testequity"->dummyobj_fc_feed_testequity)
class fc_feed_chip_digger_fc_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_chip_digger_fc(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_chip_digger_fc = new fc_feed_chip_digger_fc_converter()
converterMap+=("fc_feed_chip_digger_fc"->dummyobj_fc_feed_chip_digger_fc)
class fc_feed_rs_components_fr_mft_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_fr_mft_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_fr_mft_bnl = new fc_feed_rs_components_fr_mft_bnl_converter()
converterMap+=("fc_feed_rs_components_fr_mft_bnl"->dummyobj_fc_feed_rs_components_fr_mft_bnl)
class fc_feed_rutronik_global_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rutronik_global(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rutronik_global = new fc_feed_rutronik_global_converter()
converterMap+=("fc_feed_rutronik_global"->dummyobj_fc_feed_rutronik_global)
class fc_feed_greenchips_fc_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_greenchips_fc(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_greenchips_fc = new fc_feed_greenchips_fc_converter()
converterMap+=("fc_feed_greenchips_fc"->dummyobj_fc_feed_greenchips_fc)
class fc_feed_heisener_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_heisener(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_heisener = new fc_feed_heisener_converter()
converterMap+=("fc_feed_heisener"->dummyobj_fc_feed_heisener)
class fc_feed_aicreer_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_aicreer(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_aicreer = new fc_feed_aicreer_converter()
converterMap+=("fc_feed_aicreer"->dummyobj_fc_feed_aicreer)
class fc_feed_avnet_americas_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_avnet_americas(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_avnet_americas = new fc_feed_avnet_americas_converter()
converterMap+=("fc_feed_avnet_americas"->dummyobj_fc_feed_avnet_americas)
class fc_feed_peigenesis_005_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_peigenesis_005_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_peigenesis_005_bnl = new fc_feed_peigenesis_005_bnl_converter()
converterMap+=("fc_feed_peigenesis_005_bnl"->dummyobj_fc_feed_peigenesis_005_bnl)
class fc_feed_teconn_dynamic_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_teconn_dynamic_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_teconn_dynamic_bnl = new fc_feed_teconn_dynamic_bnl_converter()
converterMap+=("fc_feed_teconn_dynamic_bnl"->dummyobj_fc_feed_teconn_dynamic_bnl)
class fc_feed_rs_components_de_mft_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_de_mft_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_de_mft_bnl = new fc_feed_rs_components_de_mft_bnl_converter()
converterMap+=("fc_feed_rs_components_de_mft_bnl"->dummyobj_fc_feed_rs_components_de_mft_bnl)
class fc_feed_jdcomp_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_jdcomp(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_jdcomp = new fc_feed_jdcomp_converter()
converterMap+=("fc_feed_jdcomp"->dummyobj_fc_feed_jdcomp)
class fc_feed_wuhan_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_wuhan(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_wuhan = new fc_feed_wuhan_converter()
converterMap+=("fc_feed_wuhan"->dummyobj_fc_feed_wuhan)
class fc_feed_anterwell_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_anterwell(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_anterwell = new fc_feed_anterwell_converter()
converterMap+=("fc_feed_anterwell"->dummyobj_fc_feed_anterwell)
class fc_feed_rochester_ru_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rochester_ru(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rochester_ru = new fc_feed_rochester_ru_converter()
converterMap+=("fc_feed_rochester_ru"->dummyobj_fc_feed_rochester_ru)
class fc_feed_besttech_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_besttech(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_besttech = new fc_feed_besttech_converter()
converterMap+=("fc_feed_besttech"->dummyobj_fc_feed_besttech)
class fc_feed_cplus_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_cplus(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_cplus = new fc_feed_cplus_converter()
converterMap+=("fc_feed_cplus"->dummyobj_fc_feed_cplus)
class fc_feed_tme_bnl_pl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_tme_bnl_pl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_tme_bnl_pl = new fc_feed_tme_bnl_pl_converter()
converterMap+=("fc_feed_tme_bnl_pl"->dummyobj_fc_feed_tme_bnl_pl)
class fc_feed_rspro_uk_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rspro_uk(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rspro_uk = new fc_feed_rspro_uk_converter()
converterMap+=("fc_feed_rspro_uk"->dummyobj_fc_feed_rspro_uk)
class fc_feed_powersignal_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_powersignal(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_powersignal = new fc_feed_powersignal_converter()
converterMap+=("fc_feed_powersignal"->dummyobj_fc_feed_powersignal)
class fc_feed_resistortoday_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_resistortoday(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_resistortoday = new fc_feed_resistortoday_converter()
converterMap+=("fc_feed_resistortoday"->dummyobj_fc_feed_resistortoday)
class fc_feed_besatech_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_besatech(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_besatech = new fc_feed_besatech_converter()
converterMap+=("fc_feed_besatech"->dummyobj_fc_feed_besatech)
class fc_feed_corestaff_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_corestaff(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_corestaff = new fc_feed_corestaff_converter()
converterMap+=("fc_feed_corestaff"->dummyobj_fc_feed_corestaff)
class fc_feed_fudatonghe_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_fudatonghe(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_fudatonghe = new fc_feed_fudatonghe_converter()
converterMap+=("fc_feed_fudatonghe"->dummyobj_fc_feed_fudatonghe)
class fc_feed_greenchips_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_greenchips(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_greenchips = new fc_feed_greenchips_converter()
converterMap+=("fc_feed_greenchips"->dummyobj_fc_feed_greenchips)
class fc_feed_farnell_pl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_farnell_pl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_farnell_pl = new fc_feed_farnell_pl_converter()
converterMap+=("fc_feed_farnell_pl"->dummyobj_fc_feed_farnell_pl)
class fc_feed_dst_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_dst(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_dst = new fc_feed_dst_converter()
converterMap+=("fc_feed_dst"->dummyobj_fc_feed_dst)
class fc_feed_rochester_br_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rochester_br_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rochester_br_bnl = new fc_feed_rochester_br_bnl_converter()
converterMap+=("fc_feed_rochester_br_bnl"->dummyobj_fc_feed_rochester_br_bnl)
class fc_feed_elfaro_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_elfaro(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_elfaro = new fc_feed_elfaro_converter()
converterMap+=("fc_feed_elfaro"->dummyobj_fc_feed_elfaro)
class fc_feed_distrelec_it_findchips_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_distrelec_it_findchips(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_distrelec_it_findchips = new fc_feed_distrelec_it_findchips_converter()
converterMap+=("fc_feed_distrelec_it_findchips"->dummyobj_fc_feed_distrelec_it_findchips)
class fc_feed_rs_components_th_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_th(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_th = new fc_feed_rs_components_th_converter()
converterMap+=("fc_feed_rs_components_th"->dummyobj_fc_feed_rs_components_th)
class fc_feed_chipone_fc_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_chipone_fc(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_chipone_fc = new fc_feed_chipone_fc_converter()
converterMap+=("fc_feed_chipone_fc"->dummyobj_fc_feed_chipone_fc)
class fc_feed_rs_components_jp_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_jp(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_jp = new fc_feed_rs_components_jp_converter()
converterMap+=("fc_feed_rs_components_jp"->dummyobj_fc_feed_rs_components_jp)
class fc_feed_ntemall_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_ntemall(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_ntemall = new fc_feed_ntemall_converter()
converterMap+=("fc_feed_ntemall"->dummyobj_fc_feed_ntemall)
class fc_feed_chipspulse_fc_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_chipspulse_fc(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_chipspulse_fc = new fc_feed_chipspulse_fc_converter()
converterMap+=("fc_feed_chipspulse_fc"->dummyobj_fc_feed_chipspulse_fc)
class fc_feed_rspro_tr_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rspro_tr(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rspro_tr = new fc_feed_rspro_tr_converter()
converterMap+=("fc_feed_rspro_tr"->dummyobj_fc_feed_rspro_tr)
class fc_feed_spartelec_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_spartelec(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_spartelec = new fc_feed_spartelec_converter()
converterMap+=("fc_feed_spartelec"->dummyobj_fc_feed_spartelec)
class fc_feed_peigenesis_004_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_peigenesis_004(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_peigenesis_004 = new fc_feed_peigenesis_004_converter()
converterMap+=("fc_feed_peigenesis_004"->dummyobj_fc_feed_peigenesis_004)
class fc_feed_newark_rp_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_newark_rp_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_newark_rp_bnl = new fc_feed_newark_rp_bnl_converter()
converterMap+=("fc_feed_newark_rp_bnl"->dummyobj_fc_feed_newark_rp_bnl)
class fc_feed_distrelec_at_oemstrade_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_distrelec_at_oemstrade(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_distrelec_at_oemstrade = new fc_feed_distrelec_at_oemstrade_converter()
converterMap+=("fc_feed_distrelec_at_oemstrade"->dummyobj_fc_feed_distrelec_at_oemstrade)
class fc_feed_farnell_lv_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_farnell_lv(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_farnell_lv = new fc_feed_farnell_lv_converter()
converterMap+=("fc_feed_farnell_lv"->dummyobj_fc_feed_farnell_lv)
class fc_feed_corestaff_findchips_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_corestaff_findchips(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_corestaff_findchips = new fc_feed_corestaff_findchips_converter()
converterMap+=("fc_feed_corestaff_findchips"->dummyobj_fc_feed_corestaff_findchips)
class fc_feed_heilind_americas_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_heilind_americas(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_heilind_americas = new fc_feed_heilind_americas_converter()
converterMap+=("fc_feed_heilind_americas"->dummyobj_fc_feed_heilind_americas)
class fc_feed_comsit_fc_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_comsit_fc(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_comsit_fc = new fc_feed_comsit_fc_converter()
converterMap+=("fc_feed_comsit_fc"->dummyobj_fc_feed_comsit_fc)
class fc_feed_rs_components_ch_mft_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_ch_mft_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_ch_mft_bnl = new fc_feed_rs_components_ch_mft_bnl_converter()
converterMap+=("fc_feed_rs_components_ch_mft_bnl"->dummyobj_fc_feed_rs_components_ch_mft_bnl)
class fc_feed_rs_americas_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_americas_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_americas_bnl = new fc_feed_rs_americas_bnl_converter()
converterMap+=("fc_feed_rs_americas_bnl"->dummyobj_fc_feed_rs_americas_bnl)
class fc_feed_flychips_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_flychips(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_flychips = new fc_feed_flychips_converter()
converterMap+=("fc_feed_flychips"->dummyobj_fc_feed_flychips)
class fc_feed_dovecomp_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_dovecomp(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_dovecomp = new fc_feed_dovecomp_converter()
converterMap+=("fc_feed_dovecomp"->dummyobj_fc_feed_dovecomp)
class fc_feed_unibetter_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_unibetter(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_unibetter = new fc_feed_unibetter_converter()
converterMap+=("fc_feed_unibetter"->dummyobj_fc_feed_unibetter)
class fc_feed_rs_components_it_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_it(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_it = new fc_feed_rs_components_it_converter()
converterMap+=("fc_feed_rs_components_it"->dummyobj_fc_feed_rs_components_it)
class fc_feed_arrow_bnl_3_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_arrow_bnl_3(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_arrow_bnl_3 = new fc_feed_arrow_bnl_3_converter()
converterMap+=("fc_feed_arrow_bnl_3"->dummyobj_fc_feed_arrow_bnl_3)
class fc_feed_peigenesis_010_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_peigenesis_010(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_peigenesis_010 = new fc_feed_peigenesis_010_converter()
converterMap+=("fc_feed_peigenesis_010"->dummyobj_fc_feed_peigenesis_010)
class fc_feed_comsit_fc_ru_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_comsit_fc_ru(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_comsit_fc_ru = new fc_feed_comsit_fc_ru_converter()
converterMap+=("fc_feed_comsit_fc_ru"->dummyobj_fc_feed_comsit_fc_ru)
class fc_feed_lanka_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_lanka(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_lanka = new fc_feed_lanka_converter()
converterMap+=("fc_feed_lanka"->dummyobj_fc_feed_lanka)
class fc_feed_comsit_oems_es_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_comsit_oems_es(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_comsit_oems_es = new fc_feed_comsit_oems_es_converter()
converterMap+=("fc_feed_comsit_oems_es"->dummyobj_fc_feed_comsit_oems_es)
class fc_feed_future_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_future(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_future = new fc_feed_future_converter()
converterMap+=("fc_feed_future"->dummyobj_fc_feed_future)
class fc_feed_macroquest_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_macroquest(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_macroquest = new fc_feed_macroquest_converter()
converterMap+=("fc_feed_macroquest"->dummyobj_fc_feed_macroquest)
class fc_feed_compsearch_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_compsearch(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_compsearch = new fc_feed_compsearch_converter()
converterMap+=("fc_feed_compsearch"->dummyobj_fc_feed_compsearch)
class fc_feed_heqingelec_fc_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_heqingelec_fc(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_heqingelec_fc = new fc_feed_heqingelec_fc_converter()
converterMap+=("fc_feed_heqingelec_fc"->dummyobj_fc_feed_heqingelec_fc)
class fc_feed_distrelec_lv_oemstrade_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_distrelec_lv_oemstrade(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_distrelec_lv_oemstrade = new fc_feed_distrelec_lv_oemstrade_converter()
converterMap+=("fc_feed_distrelec_lv_oemstrade"->dummyobj_fc_feed_distrelec_lv_oemstrade)
class fc_feed_burklin_findchips_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_burklin_findchips(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_burklin_findchips = new fc_feed_burklin_findchips_converter()
converterMap+=("fc_feed_burklin_findchips"->dummyobj_fc_feed_burklin_findchips)
class fc_feed_unicom_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_unicom(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_unicom = new fc_feed_unicom_converter()
converterMap+=("fc_feed_unicom"->dummyobj_fc_feed_unicom)
class fc_feed_distrelec_be_findchips_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_distrelec_be_findchips(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_distrelec_be_findchips = new fc_feed_distrelec_be_findchips_converter()
converterMap+=("fc_feed_distrelec_be_findchips"->dummyobj_fc_feed_distrelec_be_findchips)
class fc_feed_farnell_hp_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_farnell_hp_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_farnell_hp_bnl = new fc_feed_farnell_hp_bnl_converter()
converterMap+=("fc_feed_farnell_hp_bnl"->dummyobj_fc_feed_farnell_hp_bnl)
class fc_feed_comsit_oems_de_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_comsit_oems_de(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_comsit_oems_de = new fc_feed_comsit_oems_de_converter()
converterMap+=("fc_feed_comsit_oems_de"->dummyobj_fc_feed_comsit_oems_de)
class fc_feed_xinyixin_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_xinyixin(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_xinyixin = new fc_feed_xinyixin_converter()
converterMap+=("fc_feed_xinyixin"->dummyobj_fc_feed_xinyixin)
class fc_feed_perfectparts_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_perfectparts(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_perfectparts = new fc_feed_perfectparts_converter()
converterMap+=("fc_feed_perfectparts"->dummyobj_fc_feed_perfectparts)
class fc_feed_tasnme_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_tasnme(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_tasnme = new fc_feed_tasnme_converter()
converterMap+=("fc_feed_tasnme"->dummyobj_fc_feed_tasnme)
class fc_feed_chip1stop_usd_oems_2_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_chip1stop_usd_oems_2(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_chip1stop_usd_oems_2 = new fc_feed_chip1stop_usd_oems_2_converter()
converterMap+=("fc_feed_chip1stop_usd_oems_2"->dummyobj_fc_feed_chip1stop_usd_oems_2)
class fc_feed_rs_components_za_mft_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_za_mft_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_za_mft_bnl = new fc_feed_rs_components_za_mft_bnl_converter()
converterMap+=("fc_feed_rs_components_za_mft_bnl"->dummyobj_fc_feed_rs_components_za_mft_bnl)
class fc_feed_b2b_etron_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_b2b_etron(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_b2b_etron = new fc_feed_b2b_etron_converter()
converterMap+=("fc_feed_b2b_etron"->dummyobj_fc_feed_b2b_etron)
class fc_feed_b2b_corestaff_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_b2b_corestaff(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_b2b_corestaff = new fc_feed_b2b_corestaff_converter()
converterMap+=("fc_feed_b2b_corestaff"->dummyobj_fc_feed_b2b_corestaff)
class fc_feed_karmieltech_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_karmieltech(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_karmieltech = new fc_feed_karmieltech_converter()
converterMap+=("fc_feed_karmieltech"->dummyobj_fc_feed_karmieltech)
class fc_feed_avnet_europe_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_avnet_europe_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_avnet_europe_bnl = new fc_feed_avnet_europe_bnl_converter()
converterMap+=("fc_feed_avnet_europe_bnl"->dummyobj_fc_feed_avnet_europe_bnl)
class fc_feed_analog_devices_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_analog_devices(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_analog_devices = new fc_feed_analog_devices_converter()
converterMap+=("fc_feed_analog_devices"->dummyobj_fc_feed_analog_devices)
class fc_feed_chipstock_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_chipstock(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_chipstock = new fc_feed_chipstock_converter()
converterMap+=("fc_feed_chipstock"->dummyobj_fc_feed_chipstock)
class fc_feed_xidaelec_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_xidaelec(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_xidaelec = new fc_feed_xidaelec_converter()
converterMap+=("fc_feed_xidaelec"->dummyobj_fc_feed_xidaelec)
class fc_feed_distrelec_no_oemstrade_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_distrelec_no_oemstrade(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_distrelec_no_oemstrade = new fc_feed_distrelec_no_oemstrade_converter()
converterMap+=("fc_feed_distrelec_no_oemstrade"->dummyobj_fc_feed_distrelec_no_oemstrade)
class fc_feed_megastar_fc_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_megastar_fc(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_megastar_fc = new fc_feed_megastar_fc_converter()
converterMap+=("fc_feed_megastar_fc"->dummyobj_fc_feed_megastar_fc)
class fc_feed_oneyac_global_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_oneyac_global(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_oneyac_global = new fc_feed_oneyac_global_converter()
converterMap+=("fc_feed_oneyac_global"->dummyobj_fc_feed_oneyac_global)
class fc_feed_tgmicro_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_tgmicro(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_tgmicro = new fc_feed_tgmicro_converter()
converterMap+=("fc_feed_tgmicro"->dummyobj_fc_feed_tgmicro)
class fc_feed_szlcsc_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_szlcsc(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_szlcsc = new fc_feed_szlcsc_converter()
converterMap+=("fc_feed_szlcsc"->dummyobj_fc_feed_szlcsc)
class fc_feed_newadvantage_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_newadvantage(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_newadvantage = new fc_feed_newadvantage_converter()
converterMap+=("fc_feed_newadvantage"->dummyobj_fc_feed_newadvantage)
class fc_feed_minamikaze_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_minamikaze(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_minamikaze = new fc_feed_minamikaze_converter()
converterMap+=("fc_feed_minamikaze"->dummyobj_fc_feed_minamikaze)
class fc_feed_lixinc_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_lixinc(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_lixinc = new fc_feed_lixinc_converter()
converterMap+=("fc_feed_lixinc"->dummyobj_fc_feed_lixinc)
class fc_feed_iceasy_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_iceasy(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_iceasy = new fc_feed_iceasy_converter()
converterMap+=("fc_feed_iceasy"->dummyobj_fc_feed_iceasy)
class fc_feed_arrow_apac_capacitors_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_arrow_apac_capacitors_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_arrow_apac_capacitors_bnl = new fc_feed_arrow_apac_capacitors_bnl_converter()
converterMap+=("fc_feed_arrow_apac_capacitors_bnl"->dummyobj_fc_feed_arrow_apac_capacitors_bnl)
class fc_feed_farnell_cz_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_farnell_cz(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_farnell_cz = new fc_feed_farnell_cz_converter()
converterMap+=("fc_feed_farnell_cz"->dummyobj_fc_feed_farnell_cz)
class fc_feed_comsit_oems_us_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_comsit_oems_us(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_comsit_oems_us = new fc_feed_comsit_oems_us_converter()
converterMap+=("fc_feed_comsit_oems_us"->dummyobj_fc_feed_comsit_oems_us)
class fc_feed_teledyne_e2v_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_teledyne_e2v(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_teledyne_e2v = new fc_feed_teledyne_e2v_converter()
converterMap+=("fc_feed_teledyne_e2v"->dummyobj_fc_feed_teledyne_e2v)
class fc_feed_distrelec_at_findchips_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_distrelec_at_findchips(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_distrelec_at_findchips = new fc_feed_distrelec_at_findchips_converter()
converterMap+=("fc_feed_distrelec_at_findchips"->dummyobj_fc_feed_distrelec_at_findchips)
class fc_feed_ztz_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_ztz(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_ztz = new fc_feed_ztz_converter()
converterMap+=("fc_feed_ztz"->dummyobj_fc_feed_ztz)
class fc_feed_wanlianxin_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_wanlianxin(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_wanlianxin = new fc_feed_wanlianxin_converter()
converterMap+=("fc_feed_wanlianxin"->dummyobj_fc_feed_wanlianxin)
class fc_feed_avnet_europe_silica_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_avnet_europe_silica(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_avnet_europe_silica = new fc_feed_avnet_europe_silica_converter()
converterMap+=("fc_feed_avnet_europe_silica"->dummyobj_fc_feed_avnet_europe_silica)
class fc_feed_farnell_ch_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_farnell_ch(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_farnell_ch = new fc_feed_farnell_ch_converter()
converterMap+=("fc_feed_farnell_ch"->dummyobj_fc_feed_farnell_ch)
class fc_feed_peigenesis_006_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_peigenesis_006_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_peigenesis_006_bnl = new fc_feed_peigenesis_006_bnl_converter()
converterMap+=("fc_feed_peigenesis_006_bnl"->dummyobj_fc_feed_peigenesis_006_bnl)
class fc_feed_icsoeasy_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_icsoeasy(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_icsoeasy = new fc_feed_icsoeasy_converter()
converterMap+=("fc_feed_icsoeasy"->dummyobj_fc_feed_icsoeasy)
class fc_feed_nacsemi_oems_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_nacsemi_oems(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_nacsemi_oems = new fc_feed_nacsemi_oems_converter()
converterMap+=("fc_feed_nacsemi_oems"->dummyobj_fc_feed_nacsemi_oems)
class fc_feed_global_solutions_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_global_solutions(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_global_solutions = new fc_feed_global_solutions_converter()
converterMap+=("fc_feed_global_solutions"->dummyobj_fc_feed_global_solutions)
class fc_feed_rs_components_at_mft_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_at_mft_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_at_mft_bnl = new fc_feed_rs_components_at_mft_bnl_converter()
converterMap+=("fc_feed_rs_components_at_mft_bnl"->dummyobj_fc_feed_rs_components_at_mft_bnl)
class fc_feed_avnet_europe_ebv_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_avnet_europe_ebv(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_avnet_europe_ebv = new fc_feed_avnet_europe_ebv_converter()
converterMap+=("fc_feed_avnet_europe_ebv"->dummyobj_fc_feed_avnet_europe_ebv)
class fc_feed_chip1stop_eur_oems_3_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_chip1stop_eur_oems_3(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_chip1stop_eur_oems_3 = new fc_feed_chip1stop_eur_oems_3_converter()
converterMap+=("fc_feed_chip1stop_eur_oems_3"->dummyobj_fc_feed_chip1stop_eur_oems_3)
class fc_feed_comsit_oems_in_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_comsit_oems_in(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_comsit_oems_in = new fc_feed_comsit_oems_in_converter()
converterMap+=("fc_feed_comsit_oems_in"->dummyobj_fc_feed_comsit_oems_in)
class fc_feed_esonic_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_esonic(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_esonic = new fc_feed_esonic_converter()
converterMap+=("fc_feed_esonic"->dummyobj_fc_feed_esonic)
class fc_feed_rspro_es_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rspro_es(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rspro_es = new fc_feed_rspro_es_converter()
converterMap+=("fc_feed_rspro_es"->dummyobj_fc_feed_rspro_es)
class fc_feed_tme_bnl_cz_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_tme_bnl_cz(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_tme_bnl_cz = new fc_feed_tme_bnl_cz_converter()
converterMap+=("fc_feed_tme_bnl_cz"->dummyobj_fc_feed_tme_bnl_cz)
class fc_feed_distrelec_fi_oemstrade_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_distrelec_fi_oemstrade(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_distrelec_fi_oemstrade = new fc_feed_distrelec_fi_oemstrade_converter()
converterMap+=("fc_feed_distrelec_fi_oemstrade"->dummyobj_fc_feed_distrelec_fi_oemstrade)
class fc_feed_rs_components_pl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_pl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_pl = new fc_feed_rs_components_pl_converter()
converterMap+=("fc_feed_rs_components_pl"->dummyobj_fc_feed_rs_components_pl)
class fc_feed_distrelec_ch_oemstrade_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_distrelec_ch_oemstrade(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_distrelec_ch_oemstrade = new fc_feed_distrelec_ch_oemstrade_converter()
converterMap+=("fc_feed_distrelec_ch_oemstrade"->dummyobj_fc_feed_distrelec_ch_oemstrade)
class fc_feed_velocity_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_velocity(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_velocity = new fc_feed_velocity_converter()
converterMap+=("fc_feed_velocity"->dummyobj_fc_feed_velocity)
class fc_feed_componentsolutions_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_componentsolutions(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_componentsolutions = new fc_feed_componentsolutions_converter()
converterMap+=("fc_feed_componentsolutions"->dummyobj_fc_feed_componentsolutions)
class fc_feed_macroship_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_macroship(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_macroship = new fc_feed_macroship_converter()
converterMap+=("fc_feed_macroship"->dummyobj_fc_feed_macroship)
class fc_feed_avnet_japan_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_avnet_japan(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_avnet_japan = new fc_feed_avnet_japan_converter()
converterMap+=("fc_feed_avnet_japan"->dummyobj_fc_feed_avnet_japan)
class fc_feed_ibuyxs_fc_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_ibuyxs_fc(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_ibuyxs_fc = new fc_feed_ibuyxs_fc_converter()
converterMap+=("fc_feed_ibuyxs_fc"->dummyobj_fc_feed_ibuyxs_fc)
class fc_feed_chipspulse_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_chipspulse_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_chipspulse_bnl = new fc_feed_chipspulse_bnl_converter()
converterMap+=("fc_feed_chipspulse_bnl"->dummyobj_fc_feed_chipspulse_bnl)
class fc_feed_distrelec_lv_findchips_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_distrelec_lv_findchips(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_distrelec_lv_findchips = new fc_feed_distrelec_lv_findchips_converter()
converterMap+=("fc_feed_distrelec_lv_findchips"->dummyobj_fc_feed_distrelec_lv_findchips)
class fc_feed_element14_my_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_element14_my(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_element14_my = new fc_feed_element14_my_converter()
converterMap+=("fc_feed_element14_my"->dummyobj_fc_feed_element14_my)
class fc_feed_farnell_fcpro_emea_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_farnell_fcpro_emea(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_farnell_fcpro_emea = new fc_feed_farnell_fcpro_emea_converter()
converterMap+=("fc_feed_farnell_fcpro_emea"->dummyobj_fc_feed_farnell_fcpro_emea)
class fc_feed_spectrum_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_spectrum(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_spectrum = new fc_feed_spectrum_converter()
converterMap+=("fc_feed_spectrum"->dummyobj_fc_feed_spectrum)
class fc_feed_chip1stop_hk_tw_kr_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_chip1stop_hk_tw_kr(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_chip1stop_hk_tw_kr = new fc_feed_chip1stop_hk_tw_kr_converter()
converterMap+=("fc_feed_chip1stop_hk_tw_kr"->dummyobj_fc_feed_chip1stop_hk_tw_kr)
class fc_feed_distrelec_de_oemstrade_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_distrelec_de_oemstrade(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_distrelec_de_oemstrade = new fc_feed_distrelec_de_oemstrade_converter()
converterMap+=("fc_feed_distrelec_de_oemstrade"->dummyobj_fc_feed_distrelec_de_oemstrade)
class fc_feed_rxelec_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rxelec(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rxelec = new fc_feed_rxelec_converter()
converterMap+=("fc_feed_rxelec"->dummyobj_fc_feed_rxelec)
class fc_feed_rs_components_ie_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_ie(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_ie = new fc_feed_rs_components_ie_converter()
converterMap+=("fc_feed_rs_components_ie"->dummyobj_fc_feed_rs_components_ie)
class fc_feed_peigenesis_007_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_peigenesis_007(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_peigenesis_007 = new fc_feed_peigenesis_007_converter()
converterMap+=("fc_feed_peigenesis_007"->dummyobj_fc_feed_peigenesis_007)
class fc_feed_commoditycomp_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_commoditycomp(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_commoditycomp = new fc_feed_commoditycomp_converter()
converterMap+=("fc_feed_commoditycomp"->dummyobj_fc_feed_commoditycomp)
class fc_feed_nxp_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_nxp(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_nxp = new fc_feed_nxp_converter()
converterMap+=("fc_feed_nxp"->dummyobj_fc_feed_nxp)
class fc_feed_chip1cloud_fc_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_chip1cloud_fc(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_chip1cloud_fc = new fc_feed_chip1cloud_fc_converter()
converterMap+=("fc_feed_chip1cloud_fc"->dummyobj_fc_feed_chip1cloud_fc)
class fc_feed_symmetry_fc_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_symmetry_fc(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_symmetry_fc = new fc_feed_symmetry_fc_converter()
converterMap+=("fc_feed_symmetry_fc"->dummyobj_fc_feed_symmetry_fc)
class fc_feed_heilind_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_heilind_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_heilind_bnl = new fc_feed_heilind_bnl_converter()
converterMap+=("fc_feed_heilind_bnl"->dummyobj_fc_feed_heilind_bnl)
class fc_feed_melchioni_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_melchioni(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_melchioni = new fc_feed_melchioni_converter()
converterMap+=("fc_feed_melchioni"->dummyobj_fc_feed_melchioni)
class fc_feed_intercard_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_intercard(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_intercard = new fc_feed_intercard_converter()
converterMap+=("fc_feed_intercard"->dummyobj_fc_feed_intercard)
class fc_feed_egbtech_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_egbtech(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_egbtech = new fc_feed_egbtech_converter()
converterMap+=("fc_feed_egbtech"->dummyobj_fc_feed_egbtech)
class fc_feed_greenlight_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_greenlight(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_greenlight = new fc_feed_greenlight_converter()
converterMap+=("fc_feed_greenlight"->dummyobj_fc_feed_greenlight)
class fc_feed_semix_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_semix(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_semix = new fc_feed_semix_converter()
converterMap+=("fc_feed_semix"->dummyobj_fc_feed_semix)
class fc_feed_tobyelec_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_tobyelec(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_tobyelec = new fc_feed_tobyelec_converter()
converterMap+=("fc_feed_tobyelec"->dummyobj_fc_feed_tobyelec)
class fc_feed_distrelec_de_findchips_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_distrelec_de_findchips(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_distrelec_de_findchips = new fc_feed_distrelec_de_findchips_converter()
converterMap+=("fc_feed_distrelec_de_findchips"->dummyobj_fc_feed_distrelec_de_findchips)
class fc_feed_rspro_nl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rspro_nl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rspro_nl = new fc_feed_rspro_nl_converter()
converterMap+=("fc_feed_rspro_nl"->dummyobj_fc_feed_rspro_nl)
class fc_feed_farnell_bnl_2_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_farnell_bnl_2(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_farnell_bnl_2 = new fc_feed_farnell_bnl_2_converter()
converterMap+=("fc_feed_farnell_bnl_2"->dummyobj_fc_feed_farnell_bnl_2)
class fc_feed_teconn_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_teconn_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_teconn_bnl = new fc_feed_teconn_bnl_converter()
converterMap+=("fc_feed_teconn_bnl"->dummyobj_fc_feed_teconn_bnl)
class fc_feed_element14_th_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_element14_th(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_element14_th = new fc_feed_element14_th_converter()
converterMap+=("fc_feed_element14_th"->dummyobj_fc_feed_element14_th)
class fc_feed_peigenesis_001_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_peigenesis_001(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_peigenesis_001 = new fc_feed_peigenesis_001_converter()
converterMap+=("fc_feed_peigenesis_001"->dummyobj_fc_feed_peigenesis_001)
class fc_feed_rjcomp_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rjcomp(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rjcomp = new fc_feed_rjcomp_converter()
converterMap+=("fc_feed_rjcomp"->dummyobj_fc_feed_rjcomp)
class fc_feed_distrelec_no_findchips_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_distrelec_no_findchips(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_distrelec_no_findchips = new fc_feed_distrelec_no_findchips_converter()
converterMap+=("fc_feed_distrelec_no_findchips"->dummyobj_fc_feed_distrelec_no_findchips)
class fc_feed_rs_components_cn_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_cn(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_cn = new fc_feed_rs_components_cn_converter()
converterMap+=("fc_feed_rs_components_cn"->dummyobj_fc_feed_rs_components_cn)
class fc_feed_avnet_bnl_americas_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_avnet_bnl_americas(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_avnet_bnl_americas = new fc_feed_avnet_bnl_americas_converter()
converterMap+=("fc_feed_avnet_bnl_americas"->dummyobj_fc_feed_avnet_bnl_americas)
class fc_feed_visioncomp_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_visioncomp(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_visioncomp = new fc_feed_visioncomp_converter()
converterMap+=("fc_feed_visioncomp"->dummyobj_fc_feed_visioncomp)
class fc_feed_chip1stop_usd_multi_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_chip1stop_usd_multi(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_chip1stop_usd_multi = new fc_feed_chip1stop_usd_multi_converter()
converterMap+=("fc_feed_chip1stop_usd_multi"->dummyobj_fc_feed_chip1stop_usd_multi)
class fc_feed_gangbo_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_gangbo(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_gangbo = new fc_feed_gangbo_converter()
converterMap+=("fc_feed_gangbo"->dummyobj_fc_feed_gangbo)
class fc_feed_wenorca_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_wenorca(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_wenorca = new fc_feed_wenorca_converter()
converterMap+=("fc_feed_wenorca"->dummyobj_fc_feed_wenorca)
class fc_feed_semihouse_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_semihouse(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_semihouse = new fc_feed_semihouse_converter()
converterMap+=("fc_feed_semihouse"->dummyobj_fc_feed_semihouse)
class fc_feed_sekorm_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_sekorm(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_sekorm = new fc_feed_sekorm_converter()
converterMap+=("fc_feed_sekorm"->dummyobj_fc_feed_sekorm)
class fc_feed_winsource_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_winsource(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_winsource = new fc_feed_winsource_converter()
converterMap+=("fc_feed_winsource"->dummyobj_fc_feed_winsource)
class fc_feed_sitime_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_sitime(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_sitime = new fc_feed_sitime_converter()
converterMap+=("fc_feed_sitime"->dummyobj_fc_feed_sitime)
class fc_feed_converge_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_converge(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_converge = new fc_feed_converge_converter()
converterMap+=("fc_feed_converge"->dummyobj_fc_feed_converge)
class fc_feed_element14_kr_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_element14_kr(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_element14_kr = new fc_feed_element14_kr_converter()
converterMap+=("fc_feed_element14_kr"->dummyobj_fc_feed_element14_kr)
class fc_feed_rs_americas_dynamic_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_americas_dynamic(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_americas_dynamic = new fc_feed_rs_americas_dynamic_converter()
converterMap+=("fc_feed_rs_americas_dynamic"->dummyobj_fc_feed_rs_americas_dynamic)
class fc_feed_future_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_future_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_future_bnl = new fc_feed_future_bnl_converter()
converterMap+=("fc_feed_future_bnl"->dummyobj_fc_feed_future_bnl)
class fc_feed_digikey_cn_b2b_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_digikey_cn_b2b(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_digikey_cn_b2b = new fc_feed_digikey_cn_b2b_converter()
converterMap+=("fc_feed_digikey_cn_b2b"->dummyobj_fc_feed_digikey_cn_b2b)
class fc_feed_distrelec_ch_findchips_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_distrelec_ch_findchips(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_distrelec_ch_findchips = new fc_feed_distrelec_ch_findchips_converter()
converterMap+=("fc_feed_distrelec_ch_findchips"->dummyobj_fc_feed_distrelec_ch_findchips)
class fc_feed_farnell_ru_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_farnell_ru(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_farnell_ru = new fc_feed_farnell_ru_converter()
converterMap+=("fc_feed_farnell_ru"->dummyobj_fc_feed_farnell_ru)
class fc_feed_chipmh_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_chipmh(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_chipmh = new fc_feed_chipmh_converter()
converterMap+=("fc_feed_chipmh"->dummyobj_fc_feed_chipmh)
class fc_feed_linkinv_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_linkinv(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_linkinv = new fc_feed_linkinv_converter()
converterMap+=("fc_feed_linkinv"->dummyobj_fc_feed_linkinv)
class fc_feed_kenawang_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_kenawang(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_kenawang = new fc_feed_kenawang_converter()
converterMap+=("fc_feed_kenawang"->dummyobj_fc_feed_kenawang)
class fc_feed_rspro_fi_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rspro_fi(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rspro_fi = new fc_feed_rspro_fi_converter()
converterMap+=("fc_feed_rspro_fi"->dummyobj_fc_feed_rspro_fi)
class fc_feed_zxd_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_zxd(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_zxd = new fc_feed_zxd_converter()
converterMap+=("fc_feed_zxd"->dummyobj_fc_feed_zxd)
class fc_feed_rspro_fr_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rspro_fr(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rspro_fr = new fc_feed_rspro_fr_converter()
converterMap+=("fc_feed_rspro_fr"->dummyobj_fc_feed_rspro_fr)
class fc_feed_avnet_asia_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_avnet_asia(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_avnet_asia = new fc_feed_avnet_asia_converter()
converterMap+=("fc_feed_avnet_asia"->dummyobj_fc_feed_avnet_asia)
class fc_feed_sensible_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_sensible(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_sensible = new fc_feed_sensible_converter()
converterMap+=("fc_feed_sensible"->dummyobj_fc_feed_sensible)
class fc_feed_hongxinwei_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_hongxinwei(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_hongxinwei = new fc_feed_hongxinwei_converter()
converterMap+=("fc_feed_hongxinwei"->dummyobj_fc_feed_hongxinwei)
class fc_feed_amplechip_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_amplechip(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_amplechip = new fc_feed_amplechip_converter()
converterMap+=("fc_feed_amplechip"->dummyobj_fc_feed_amplechip)
class fc_feed_distrelec_fi_findchips_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_distrelec_fi_findchips(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_distrelec_fi_findchips = new fc_feed_distrelec_fi_findchips_converter()
converterMap+=("fc_feed_distrelec_fi_findchips"->dummyobj_fc_feed_distrelec_fi_findchips)
class fc_feed_escomp_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_escomp(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_escomp = new fc_feed_escomp_converter()
converterMap+=("fc_feed_escomp"->dummyobj_fc_feed_escomp)
class fc_feed_hkdcy_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_hkdcy(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_hkdcy = new fc_feed_hkdcy_converter()
converterMap+=("fc_feed_hkdcy"->dummyobj_fc_feed_hkdcy)
class fc_feed_inventorymp_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_inventorymp(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_inventorymp = new fc_feed_inventorymp_converter()
converterMap+=("fc_feed_inventorymp"->dummyobj_fc_feed_inventorymp)
class fc_feed_rs_components_za_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_za(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_za = new fc_feed_rs_components_za_converter()
converterMap+=("fc_feed_rs_components_za"->dummyobj_fc_feed_rs_components_za)
class fc_feed_nacsemi_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_nacsemi(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_nacsemi = new fc_feed_nacsemi_converter()
converterMap+=("fc_feed_nacsemi"->dummyobj_fc_feed_nacsemi)
class fc_feed_vanda_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_vanda(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_vanda = new fc_feed_vanda_converter()
converterMap+=("fc_feed_vanda"->dummyobj_fc_feed_vanda)
class fc_feed_ti_bnl_epd1_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_ti_bnl_epd1(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_ti_bnl_epd1 = new fc_feed_ti_bnl_epd1_converter()
converterMap+=("fc_feed_ti_bnl_epd1"->dummyobj_fc_feed_ti_bnl_epd1)
class fc_feed_finestock_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_finestock(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_finestock = new fc_feed_finestock_converter()
converterMap+=("fc_feed_finestock"->dummyobj_fc_feed_finestock)
class fc_feed_comsit_oems_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_comsit_oems(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_comsit_oems = new fc_feed_comsit_oems_converter()
converterMap+=("fc_feed_comsit_oems"->dummyobj_fc_feed_comsit_oems)
class fc_feed_farnell_sk_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_farnell_sk(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_farnell_sk = new fc_feed_farnell_sk_converter()
converterMap+=("fc_feed_farnell_sk"->dummyobj_fc_feed_farnell_sk)
class fc_feed_wpg_americas_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_wpg_americas(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_wpg_americas = new fc_feed_wpg_americas_converter()
converterMap+=("fc_feed_wpg_americas"->dummyobj_fc_feed_wpg_americas)
class fc_feed_distrelec_global_oemstrade_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_distrelec_global_oemstrade(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_distrelec_global_oemstrade = new fc_feed_distrelec_global_oemstrade_converter()
converterMap+=("fc_feed_distrelec_global_oemstrade"->dummyobj_fc_feed_distrelec_global_oemstrade)
class fc_feed_greentree_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_greentree(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_greentree = new fc_feed_greentree_converter()
converterMap+=("fc_feed_greentree"->dummyobj_fc_feed_greentree)
class fc_feed_rspro_dk_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rspro_dk(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rspro_dk = new fc_feed_rspro_dk_converter()
converterMap+=("fc_feed_rspro_dk"->dummyobj_fc_feed_rspro_dk)
class fc_feed_richelec_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_richelec(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_richelec = new fc_feed_richelec_converter()
converterMap+=("fc_feed_richelec"->dummyobj_fc_feed_richelec)
class fc_feed_farnell_ro_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_farnell_ro(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_farnell_ro = new fc_feed_farnell_ro_converter()
converterMap+=("fc_feed_farnell_ro"->dummyobj_fc_feed_farnell_ro)
class fc_feed_arrow_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_arrow(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_arrow = new fc_feed_arrow_converter()
converterMap+=("fc_feed_arrow"->dummyobj_fc_feed_arrow)
class fc_feed_master_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_master(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_master = new fc_feed_master_converter()
converterMap+=("fc_feed_master"->dummyobj_fc_feed_master)
class fc_feed_globaltek_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_globaltek(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_globaltek = new fc_feed_globaltek_converter()
converterMap+=("fc_feed_globaltek"->dummyobj_fc_feed_globaltek)
class fc_feed_etchips_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_etchips(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_etchips = new fc_feed_etchips_converter()
converterMap+=("fc_feed_etchips"->dummyobj_fc_feed_etchips)
class fc_feed_xsource_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_xsource(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_xsource = new fc_feed_xsource_converter()
converterMap+=("fc_feed_xsource"->dummyobj_fc_feed_xsource)
class fc_feed_rs_components_cz_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_cz(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_cz = new fc_feed_rs_components_cz_converter()
converterMap+=("fc_feed_rs_components_cz"->dummyobj_fc_feed_rs_components_cz)
class fc_feed_nacsemi_fc_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_nacsemi_fc(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_nacsemi_fc = new fc_feed_nacsemi_fc_converter()
converterMap+=("fc_feed_nacsemi_fc"->dummyobj_fc_feed_nacsemi_fc)
class fc_feed_rs_components_ch_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_ch(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_ch = new fc_feed_rs_components_ch_converter()
converterMap+=("fc_feed_rs_components_ch"->dummyobj_fc_feed_rs_components_ch)
class fc_feed_comsit_oems_it_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_comsit_oems_it(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_comsit_oems_it = new fc_feed_comsit_oems_it_converter()
converterMap+=("fc_feed_comsit_oems_it"->dummyobj_fc_feed_comsit_oems_it)
class fc_feed_rs_components_ie_mft_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_ie_mft_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_ie_mft_bnl = new fc_feed_rs_components_ie_mft_bnl_converter()
converterMap+=("fc_feed_rs_components_ie_mft_bnl"->dummyobj_fc_feed_rs_components_ie_mft_bnl)
class fc_feed_chip1stop_eur_multi_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_chip1stop_eur_multi(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_chip1stop_eur_multi = new fc_feed_chip1stop_eur_multi_converter()
converterMap+=("fc_feed_chip1stop_eur_multi"->dummyobj_fc_feed_chip1stop_eur_multi)
class fc_feed_avnet_americas_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_avnet_americas_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_avnet_americas_bnl = new fc_feed_avnet_americas_bnl_converter()
converterMap+=("fc_feed_avnet_americas_bnl"->dummyobj_fc_feed_avnet_americas_bnl)
class fc_feed_ti_bnl_app_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_ti_bnl_app(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_ti_bnl_app = new fc_feed_ti_bnl_app_converter()
converterMap+=("fc_feed_ti_bnl_app"->dummyobj_fc_feed_ti_bnl_app)
class fc_feed_rspro_no_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rspro_no(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rspro_no = new fc_feed_rspro_no_converter()
converterMap+=("fc_feed_rspro_no"->dummyobj_fc_feed_rspro_no)
class fc_feed_rs_components_nz_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_nz(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_nz = new fc_feed_rs_components_nz_converter()
converterMap+=("fc_feed_rs_components_nz"->dummyobj_fc_feed_rs_components_nz)
class fc_feed_peigenesis_us_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_peigenesis_us_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_peigenesis_us_bnl = new fc_feed_peigenesis_us_bnl_converter()
converterMap+=("fc_feed_peigenesis_us_bnl"->dummyobj_fc_feed_peigenesis_us_bnl)
class fc_feed_newstrength_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_newstrength(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_newstrength = new fc_feed_newstrength_converter()
converterMap+=("fc_feed_newstrength"->dummyobj_fc_feed_newstrength)
class fc_feed_peerless_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_peerless(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_peerless = new fc_feed_peerless_converter()
converterMap+=("fc_feed_peerless"->dummyobj_fc_feed_peerless)
class fc_feed_dgttech_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_dgttech(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_dgttech = new fc_feed_dgttech_converter()
converterMap+=("fc_feed_dgttech"->dummyobj_fc_feed_dgttech)
class fc_feed_chip1stop_jpy_oems_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_chip1stop_jpy_oems(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_chip1stop_jpy_oems = new fc_feed_chip1stop_jpy_oems_converter()
converterMap+=("fc_feed_chip1stop_jpy_oems"->dummyobj_fc_feed_chip1stop_jpy_oems)
class fc_feed_rs_components_ro_mft_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_ro_mft_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_ro_mft_bnl = new fc_feed_rs_components_ro_mft_bnl_converter()
converterMap+=("fc_feed_rs_components_ro_mft_bnl"->dummyobj_fc_feed_rs_components_ro_mft_bnl)
class fc_feed_kstcomp_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_kstcomp(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_kstcomp = new fc_feed_kstcomp_converter()
converterMap+=("fc_feed_kstcomp"->dummyobj_fc_feed_kstcomp)
class fc_feed_forward_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_forward(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_forward = new fc_feed_forward_converter()
converterMap+=("fc_feed_forward"->dummyobj_fc_feed_forward)
class fc_feed_weyland_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_weyland(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_weyland = new fc_feed_weyland_converter()
converterMap+=("fc_feed_weyland"->dummyobj_fc_feed_weyland)
class fc_feed_littletech_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_littletech(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_littletech = new fc_feed_littletech_converter()
converterMap+=("fc_feed_littletech"->dummyobj_fc_feed_littletech)
class fc_feed_molex_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_molex(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_molex = new fc_feed_molex_converter()
converterMap+=("fc_feed_molex"->dummyobj_fc_feed_molex)
class fc_feed_oneyac_oems_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_oneyac_oems(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_oneyac_oems = new fc_feed_oneyac_oems_converter()
converterMap+=("fc_feed_oneyac_oems"->dummyobj_fc_feed_oneyac_oems)
class fc_feed_zhongkaixin_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_zhongkaixin(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_zhongkaixin = new fc_feed_zhongkaixin_converter()
converterMap+=("fc_feed_zhongkaixin"->dummyobj_fc_feed_zhongkaixin)
class fc_feed_chipspulse_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_chipspulse(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_chipspulse = new fc_feed_chipspulse_converter()
converterMap+=("fc_feed_chipspulse"->dummyobj_fc_feed_chipspulse)
class fc_feed_chip1stop_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_chip1stop(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_chip1stop = new fc_feed_chip1stop_converter()
converterMap+=("fc_feed_chip1stop"->dummyobj_fc_feed_chip1stop)
class fc_feed_distrelec_se_oemstrade_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_distrelec_se_oemstrade(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_distrelec_se_oemstrade = new fc_feed_distrelec_se_oemstrade_converter()
converterMap+=("fc_feed_distrelec_se_oemstrade"->dummyobj_fc_feed_distrelec_se_oemstrade)
class fc_feed_farnell_bnl_littelfuse_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_farnell_bnl_littelfuse(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_farnell_bnl_littelfuse = new fc_feed_farnell_bnl_littelfuse_converter()
converterMap+=("fc_feed_farnell_bnl_littelfuse"->dummyobj_fc_feed_farnell_bnl_littelfuse)
class fc_feed_netsight_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_netsight(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_netsight = new fc_feed_netsight_converter()
converterMap+=("fc_feed_netsight"->dummyobj_fc_feed_netsight)
class fc_feed_btwelec_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_btwelec(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_btwelec = new fc_feed_btwelec_converter()
converterMap+=("fc_feed_btwelec"->dummyobj_fc_feed_btwelec)
class fc_feed_heilind_europe_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_heilind_europe(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_heilind_europe = new fc_feed_heilind_europe_converter()
converterMap+=("fc_feed_heilind_europe"->dummyobj_fc_feed_heilind_europe)
class fc_feed_allied_findchips_us_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_allied_findchips_us(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_allied_findchips_us = new fc_feed_allied_findchips_us_converter()
converterMap+=("fc_feed_allied_findchips_us"->dummyobj_fc_feed_allied_findchips_us)
class fc_feed_relayspec_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_relayspec(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_relayspec = new fc_feed_relayspec_converter()
converterMap+=("fc_feed_relayspec"->dummyobj_fc_feed_relayspec)
class fc_feed_electroshield_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_electroshield(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_electroshield = new fc_feed_electroshield_converter()
converterMap+=("fc_feed_electroshield"->dummyobj_fc_feed_electroshield)
class fc_feed_distrelec_cz_oemstrade_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_distrelec_cz_oemstrade(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_distrelec_cz_oemstrade = new fc_feed_distrelec_cz_oemstrade_converter()
converterMap+=("fc_feed_distrelec_cz_oemstrade"->dummyobj_fc_feed_distrelec_cz_oemstrade)
class fc_feed_ameya_oems_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_ameya_oems(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_ameya_oems = new fc_feed_ameya_oems_converter()
converterMap+=("fc_feed_ameya_oems"->dummyobj_fc_feed_ameya_oems)
class fc_feed_techdesign_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_techdesign(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_techdesign = new fc_feed_techdesign_converter()
converterMap+=("fc_feed_techdesign"->dummyobj_fc_feed_techdesign)
class fc_feed_teamgene_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_teamgene(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_teamgene = new fc_feed_teamgene_converter()
converterMap+=("fc_feed_teamgene"->dummyobj_fc_feed_teamgene)
class fc_feed_macroquest_fc_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_macroquest_fc(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_macroquest_fc = new fc_feed_macroquest_fc_converter()
converterMap+=("fc_feed_macroquest_fc"->dummyobj_fc_feed_macroquest_fc)
class fc_feed_distrelec_oems_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_distrelec_oems(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_distrelec_oems = new fc_feed_distrelec_oems_converter()
converterMap+=("fc_feed_distrelec_oems"->dummyobj_fc_feed_distrelec_oems)
class fc_feed_neutron_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_neutron(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_neutron = new fc_feed_neutron_converter()
converterMap+=("fc_feed_neutron"->dummyobj_fc_feed_neutron)
class fc_feed_amh_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_amh(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_amh = new fc_feed_amh_converter()
converterMap+=("fc_feed_amh"->dummyobj_fc_feed_amh)
class fc_feed_globx_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_globx(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_globx = new fc_feed_globx_converter()
converterMap+=("fc_feed_globx"->dummyobj_fc_feed_globx)
class fc_feed_touchstone_systems_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_touchstone_systems(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_touchstone_systems = new fc_feed_touchstone_systems_converter()
converterMap+=("fc_feed_touchstone_systems"->dummyobj_fc_feed_touchstone_systems)
class fc_feed_lukelec_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_lukelec(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_lukelec = new fc_feed_lukelec_converter()
converterMap+=("fc_feed_lukelec"->dummyobj_fc_feed_lukelec)
class fc_feed_distrelec_dk_findchips_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_distrelec_dk_findchips(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_distrelec_dk_findchips = new fc_feed_distrelec_dk_findchips_converter()
converterMap+=("fc_feed_distrelec_dk_findchips"->dummyobj_fc_feed_distrelec_dk_findchips)
class fc_feed_waytek_fc_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_waytek_fc(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_waytek_fc = new fc_feed_waytek_fc_converter()
converterMap+=("fc_feed_waytek_fc"->dummyobj_fc_feed_waytek_fc)
class fc_feed_rspro_it_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rspro_it(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rspro_it = new fc_feed_rspro_it_converter()
converterMap+=("fc_feed_rspro_it"->dummyobj_fc_feed_rspro_it)
class fc_feed_b2b_cecport_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_b2b_cecport(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_b2b_cecport = new fc_feed_b2b_cecport_converter()
converterMap+=("fc_feed_b2b_cecport"->dummyobj_fc_feed_b2b_cecport)
class fc_feed_erize_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_erize(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_erize = new fc_feed_erize_converter()
converterMap+=("fc_feed_erize"->dummyobj_fc_feed_erize)
class fc_feed_murata_cn_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_murata_cn(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_murata_cn = new fc_feed_murata_cn_converter()
converterMap+=("fc_feed_murata_cn"->dummyobj_fc_feed_murata_cn)
class fc_feed_chiefent_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_chiefent(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_chiefent = new fc_feed_chiefent_converter()
converterMap+=("fc_feed_chiefent"->dummyobj_fc_feed_chiefent)
class fc_feed_ecco_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_ecco(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_ecco = new fc_feed_ecco_converter()
converterMap+=("fc_feed_ecco"->dummyobj_fc_feed_ecco)
class fc_feed_icpartner_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_icpartner(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_icpartner = new fc_feed_icpartner_converter()
converterMap+=("fc_feed_icpartner"->dummyobj_fc_feed_icpartner)
class fc_feed_element14_tw_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_element14_tw(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_element14_tw = new fc_feed_element14_tw_converter()
converterMap+=("fc_feed_element14_tw"->dummyobj_fc_feed_element14_tw)
class fc_feed_rspro_de_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rspro_de(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rspro_de = new fc_feed_rspro_de_converter()
converterMap+=("fc_feed_rspro_de"->dummyobj_fc_feed_rspro_de)
class fc_feed_digikey_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_digikey(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_digikey = new fc_feed_digikey_converter()
converterMap+=("fc_feed_digikey"->dummyobj_fc_feed_digikey)
class fc_feed_vyrian_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_vyrian(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_vyrian = new fc_feed_vyrian_converter()
converterMap+=("fc_feed_vyrian"->dummyobj_fc_feed_vyrian)
class fc_feed_electrocraft_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_electrocraft(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_electrocraft = new fc_feed_electrocraft_converter()
converterMap+=("fc_feed_electrocraft"->dummyobj_fc_feed_electrocraft)
class fc_feed_distrelec_global_findchips_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_distrelec_global_findchips(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_distrelec_global_findchips = new fc_feed_distrelec_global_findchips_converter()
converterMap+=("fc_feed_distrelec_global_findchips"->dummyobj_fc_feed_distrelec_global_findchips)
class fc_feed_vast_global_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_vast_global(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_vast_global = new fc_feed_vast_global_converter()
converterMap+=("fc_feed_vast_global"->dummyobj_fc_feed_vast_global)
class fc_feed_arrow_bnl_smp_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_arrow_bnl_smp(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_arrow_bnl_smp = new fc_feed_arrow_bnl_smp_converter()
converterMap+=("fc_feed_arrow_bnl_smp"->dummyobj_fc_feed_arrow_bnl_smp)
class fc_feed_ichunt_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_ichunt(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_ichunt = new fc_feed_ichunt_converter()
converterMap+=("fc_feed_ichunt"->dummyobj_fc_feed_ichunt)
class fc_feed_avnet_japan_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_avnet_japan_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_avnet_japan_bnl = new fc_feed_avnet_japan_bnl_converter()
converterMap+=("fc_feed_avnet_japan_bnl"->dummyobj_fc_feed_avnet_japan_bnl)
class fc_feed_element14_in_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_element14_in(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_element14_in = new fc_feed_element14_in_converter()
converterMap+=("fc_feed_element14_in"->dummyobj_fc_feed_element14_in)
class fc_feed_electroent_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_electroent(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_electroent = new fc_feed_electroent_converter()
converterMap+=("fc_feed_electroent"->dummyobj_fc_feed_electroent)
class fc_feed_rochester_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rochester(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rochester = new fc_feed_rochester_converter()
converterMap+=("fc_feed_rochester"->dummyobj_fc_feed_rochester)
class fc_feed_rs_components_uk_mft_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_uk_mft_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_uk_mft_bnl = new fc_feed_rs_components_uk_mft_bnl_converter()
converterMap+=("fc_feed_rs_components_uk_mft_bnl"->dummyobj_fc_feed_rs_components_uk_mft_bnl)
class fc_feed_digikey_cn_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_digikey_cn(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_digikey_cn = new fc_feed_digikey_cn_converter()
converterMap+=("fc_feed_digikey_cn"->dummyobj_fc_feed_digikey_cn)
class fc_feed_newideas_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_newideas(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_newideas = new fc_feed_newideas_converter()
converterMap+=("fc_feed_newideas"->dummyobj_fc_feed_newideas)
class fc_feed_farnell_se_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_farnell_se(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_farnell_se = new fc_feed_farnell_se_converter()
converterMap+=("fc_feed_farnell_se"->dummyobj_fc_feed_farnell_se)
class fc_feed_shenghuayuan_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_shenghuayuan(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_shenghuayuan = new fc_feed_shenghuayuan_converter()
converterMap+=("fc_feed_shenghuayuan"->dummyobj_fc_feed_shenghuayuan)
class fc_feed_directcomp_fc_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_directcomp_fc(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_directcomp_fc = new fc_feed_directcomp_fc_converter()
converterMap+=("fc_feed_directcomp_fc"->dummyobj_fc_feed_directcomp_fc)
class fc_feed_comsit_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_comsit(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_comsit = new fc_feed_comsit_converter()
converterMap+=("fc_feed_comsit"->dummyobj_fc_feed_comsit)
class fc_feed_flipelec_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_flipelec(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_flipelec = new fc_feed_flipelec_converter()
converterMap+=("fc_feed_flipelec"->dummyobj_fc_feed_flipelec)
class fc_feed_element14_cn_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_element14_cn(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_element14_cn = new fc_feed_element14_cn_converter()
converterMap+=("fc_feed_element14_cn"->dummyobj_fc_feed_element14_cn)
class fc_feed_winsource_fc_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_winsource_fc(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_winsource_fc = new fc_feed_winsource_fc_converter()
converterMap+=("fc_feed_winsource_fc"->dummyobj_fc_feed_winsource_fc)
class fc_feed_comsit_available_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_comsit_available(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_comsit_available = new fc_feed_comsit_available_converter()
converterMap+=("fc_feed_comsit_available"->dummyobj_fc_feed_comsit_available)
class fc_feed_comsit_oems_cn_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_comsit_oems_cn(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_comsit_oems_cn = new fc_feed_comsit_oems_cn_converter()
converterMap+=("fc_feed_comsit_oems_cn"->dummyobj_fc_feed_comsit_oems_cn)
class fc_feed_icchipshop_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_icchipshop(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_icchipshop = new fc_feed_icchipshop_converter()
converterMap+=("fc_feed_icchipshop"->dummyobj_fc_feed_icchipshop)
class fc_feed_venatech_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_venatech(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_venatech = new fc_feed_venatech_converter()
converterMap+=("fc_feed_venatech"->dummyobj_fc_feed_venatech)
class fc_feed_future_cn_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_future_cn_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_future_cn_bnl = new fc_feed_future_cn_bnl_converter()
converterMap+=("fc_feed_future_cn_bnl"->dummyobj_fc_feed_future_cn_bnl)
class fc_feed_incielcom_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_incielcom(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_incielcom = new fc_feed_incielcom_converter()
converterMap+=("fc_feed_incielcom"->dummyobj_fc_feed_incielcom)
class fc_feed_tlc_electronics_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_tlc_electronics(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_tlc_electronics = new fc_feed_tlc_electronics_converter()
converterMap+=("fc_feed_tlc_electronics"->dummyobj_fc_feed_tlc_electronics)
class fc_feed_pasternack_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_pasternack(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_pasternack = new fc_feed_pasternack_converter()
converterMap+=("fc_feed_pasternack"->dummyobj_fc_feed_pasternack)
class fc_feed_b2b_chip1stop_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_b2b_chip1stop(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_b2b_chip1stop = new fc_feed_b2b_chip1stop_converter()
converterMap+=("fc_feed_b2b_chip1stop"->dummyobj_fc_feed_b2b_chip1stop)
class fc_feed_hongyinwei_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_hongyinwei(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_hongyinwei = new fc_feed_hongyinwei_converter()
converterMap+=("fc_feed_hongyinwei"->dummyobj_fc_feed_hongyinwei)
class fc_feed_rs_components_sk_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_sk(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_sk = new fc_feed_rs_components_sk_converter()
converterMap+=("fc_feed_rs_components_sk"->dummyobj_fc_feed_rs_components_sk)
class fc_feed_cdmelec_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_cdmelec(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_cdmelec = new fc_feed_cdmelec_converter()
converterMap+=("fc_feed_cdmelec"->dummyobj_fc_feed_cdmelec)
class fc_feed_distrelec_se_findchips_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_distrelec_se_findchips(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_distrelec_se_findchips = new fc_feed_distrelec_se_findchips_converter()
converterMap+=("fc_feed_distrelec_se_findchips"->dummyobj_fc_feed_distrelec_se_findchips)
class fc_feed_newadvantage_fc_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_newadvantage_fc(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_newadvantage_fc = new fc_feed_newadvantage_fc_converter()
converterMap+=("fc_feed_newadvantage_fc"->dummyobj_fc_feed_newadvantage_fc)
class fc_feed_chip_germany_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_chip_germany(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_chip_germany = new fc_feed_chip_germany_converter()
converterMap+=("fc_feed_chip_germany"->dummyobj_fc_feed_chip_germany)
class fc_feed_nacsemi_us_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_nacsemi_us(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_nacsemi_us = new fc_feed_nacsemi_us_converter()
converterMap+=("fc_feed_nacsemi_us"->dummyobj_fc_feed_nacsemi_us)
class fc_feed_semour_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_semour(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_semour = new fc_feed_semour_converter()
converterMap+=("fc_feed_semour"->dummyobj_fc_feed_semour)
class fc_feed_newark_hp_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_newark_hp_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_newark_hp_bnl = new fc_feed_newark_hp_bnl_converter()
converterMap+=("fc_feed_newark_hp_bnl"->dummyobj_fc_feed_newark_hp_bnl)
class fc_feed_allied_oemstrade_us_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_allied_oemstrade_us(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_allied_oemstrade_us = new fc_feed_allied_oemstrade_us_converter()
converterMap+=("fc_feed_allied_oemstrade_us"->dummyobj_fc_feed_allied_oemstrade_us)
class fc_feed_rs_components_fi_mft_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_fi_mft_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_fi_mft_bnl = new fc_feed_rs_components_fi_mft_bnl_converter()
converterMap+=("fc_feed_rs_components_fi_mft_bnl"->dummyobj_fc_feed_rs_components_fi_mft_bnl)
class fc_feed_rs_components_ro_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_ro(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_ro = new fc_feed_rs_components_ro_converter()
converterMap+=("fc_feed_rs_components_ro"->dummyobj_fc_feed_rs_components_ro)
class fc_feed_rs_components_hk_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_hk(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_hk = new fc_feed_rs_components_hk_converter()
converterMap+=("fc_feed_rs_components_hk"->dummyobj_fc_feed_rs_components_hk)
class fc_feed_flip_fc_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_flip_fc(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_flip_fc = new fc_feed_flip_fc_converter()
converterMap+=("fc_feed_flip_fc"->dummyobj_fc_feed_flip_fc)
class fc_feed_polyphaser_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_polyphaser(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_polyphaser = new fc_feed_polyphaser_converter()
converterMap+=("fc_feed_polyphaser"->dummyobj_fc_feed_polyphaser)
class fc_feed_flywing_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_flywing(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_flywing = new fc_feed_flywing_converter()
converterMap+=("fc_feed_flywing"->dummyobj_fc_feed_flywing)
class fc_feed_flyking_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_flyking(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_flyking = new fc_feed_flyking_converter()
converterMap+=("fc_feed_flyking"->dummyobj_fc_feed_flyking)
class fc_feed_component_stockers_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_component_stockers(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_component_stockers = new fc_feed_component_stockers_converter()
converterMap+=("fc_feed_component_stockers"->dummyobj_fc_feed_component_stockers)
class fc_feed_peigenesis_003_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_peigenesis_003_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_peigenesis_003_bnl = new fc_feed_peigenesis_003_bnl_converter()
converterMap+=("fc_feed_peigenesis_003_bnl"->dummyobj_fc_feed_peigenesis_003_bnl)
class fc_feed_chip1stop_cny_oems_3_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_chip1stop_cny_oems_3(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_chip1stop_cny_oems_3 = new fc_feed_chip1stop_cny_oems_3_converter()
converterMap+=("fc_feed_chip1stop_cny_oems_3"->dummyobj_fc_feed_chip1stop_cny_oems_3)
class fc_feed_kruse_fc_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_kruse_fc(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_kruse_fc = new fc_feed_kruse_fc_converter()
converterMap+=("fc_feed_kruse_fc"->dummyobj_fc_feed_kruse_fc)
class fc_feed_ystchips_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_ystchips(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_ystchips = new fc_feed_ystchips_converter()
converterMap+=("fc_feed_ystchips"->dummyobj_fc_feed_ystchips)
class fc_feed_cxda_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_cxda(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_cxda = new fc_feed_cxda_converter()
converterMap+=("fc_feed_cxda"->dummyobj_fc_feed_cxda)
class fc_feed_rspro_ie_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rspro_ie(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rspro_ie = new fc_feed_rspro_ie_converter()
converterMap+=("fc_feed_rspro_ie"->dummyobj_fc_feed_rspro_ie)
class fc_feed_rs_americas_bnl_rs_pro_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_americas_bnl_rs_pro(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_americas_bnl_rs_pro = new fc_feed_rs_americas_bnl_rs_pro_converter()
converterMap+=("fc_feed_rs_americas_bnl_rs_pro"->dummyobj_fc_feed_rs_americas_bnl_rs_pro)
class fc_feed_dynamic_fc_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_dynamic_fc(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_dynamic_fc = new fc_feed_dynamic_fc_converter()
converterMap+=("fc_feed_dynamic_fc"->dummyobj_fc_feed_dynamic_fc)
class fc_feed_rspro_at_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rspro_at(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rspro_at = new fc_feed_rspro_at_converter()
converterMap+=("fc_feed_rspro_at"->dummyobj_fc_feed_rspro_at)
class fc_feed_comsit_oems_br_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_comsit_oems_br(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_comsit_oems_br = new fc_feed_comsit_oems_br_converter()
converterMap+=("fc_feed_comsit_oems_br"->dummyobj_fc_feed_comsit_oems_br)
class fc_feed_heilind_asia_oems_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_heilind_asia_oems(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_heilind_asia_oems = new fc_feed_heilind_asia_oems_converter()
converterMap+=("fc_feed_heilind_asia_oems"->dummyobj_fc_feed_heilind_asia_oems)
class fc_feed_jrh_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_jrh(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_jrh = new fc_feed_jrh_converter()
converterMap+=("fc_feed_jrh"->dummyobj_fc_feed_jrh)
class fc_feed_vrgcomp_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_vrgcomp(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_vrgcomp = new fc_feed_vrgcomp_converter()
converterMap+=("fc_feed_vrgcomp"->dummyobj_fc_feed_vrgcomp)
class fc_feed_newark_us_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_newark_us(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_newark_us = new fc_feed_newark_us_converter()
converterMap+=("fc_feed_newark_us"->dummyobj_fc_feed_newark_us)
class fc_feed_springtech_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_springtech(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_springtech = new fc_feed_springtech_converter()
converterMap+=("fc_feed_springtech"->dummyobj_fc_feed_springtech)
class fc_feed_distrelec_cz_findchips_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_distrelec_cz_findchips(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_distrelec_cz_findchips = new fc_feed_distrelec_cz_findchips_converter()
converterMap+=("fc_feed_distrelec_cz_findchips"->dummyobj_fc_feed_distrelec_cz_findchips)
class fc_feed_waytek_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_waytek(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_waytek = new fc_feed_waytek_converter()
converterMap+=("fc_feed_waytek"->dummyobj_fc_feed_waytek)
class fc_feed_farnell_be_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_farnell_be(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_farnell_be = new fc_feed_farnell_be_converter()
converterMap+=("fc_feed_farnell_be"->dummyobj_fc_feed_farnell_be)
class fc_feed_viczone_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_viczone(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_viczone = new fc_feed_viczone_converter()
converterMap+=("fc_feed_viczone"->dummyobj_fc_feed_viczone)
class fc_feed_rs_components_be_mft_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_be_mft_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_be_mft_bnl = new fc_feed_rs_components_be_mft_bnl_converter()
converterMap+=("fc_feed_rs_components_be_mft_bnl"->dummyobj_fc_feed_rs_components_be_mft_bnl)
class fc_feed_aepetsche_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_aepetsche(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_aepetsche = new fc_feed_aepetsche_converter()
converterMap+=("fc_feed_aepetsche"->dummyobj_fc_feed_aepetsche)
class fc_feed_bitfoic_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_bitfoic(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_bitfoic = new fc_feed_bitfoic_converter()
converterMap+=("fc_feed_bitfoic"->dummyobj_fc_feed_bitfoic)
class fc_feed_newadvantage_oems_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_newadvantage_oems(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_newadvantage_oems = new fc_feed_newadvantage_oems_converter()
converterMap+=("fc_feed_newadvantage_oems"->dummyobj_fc_feed_newadvantage_oems)
class fc_feed_aps_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_aps(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_aps = new fc_feed_aps_converter()
converterMap+=("fc_feed_aps"->dummyobj_fc_feed_aps)
class fc_feed_farnell_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_farnell(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_farnell = new fc_feed_farnell_converter()
converterMap+=("fc_feed_farnell"->dummyobj_fc_feed_farnell)
class fc_feed_westcoast_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_westcoast(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_westcoast = new fc_feed_westcoast_converter()
converterMap+=("fc_feed_westcoast"->dummyobj_fc_feed_westcoast)
class fc_feed_rspro_hu_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rspro_hu(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rspro_hu = new fc_feed_rspro_hu_converter()
converterMap+=("fc_feed_rspro_hu"->dummyobj_fc_feed_rspro_hu)
class fc_feed_microchip_usa_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_microchip_usa(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_microchip_usa = new fc_feed_microchip_usa_converter()
converterMap+=("fc_feed_microchip_usa"->dummyobj_fc_feed_microchip_usa)
class fc_feed_farnell_ee_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_farnell_ee(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_farnell_ee = new fc_feed_farnell_ee_converter()
converterMap+=("fc_feed_farnell_ee"->dummyobj_fc_feed_farnell_ee)
class fc_feed_vyrian_fc_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_vyrian_fc(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_vyrian_fc = new fc_feed_vyrian_fc_converter()
converterMap+=("fc_feed_vyrian_fc"->dummyobj_fc_feed_vyrian_fc)
class fc_feed_samno_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_samno(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_samno = new fc_feed_samno_converter()
converterMap+=("fc_feed_samno"->dummyobj_fc_feed_samno)
class fc_feed_tme_oems_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_tme_oems(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_tme_oems = new fc_feed_tme_oems_converter()
converterMap+=("fc_feed_tme_oems"->dummyobj_fc_feed_tme_oems)
class fc_feed_avnet_dp_nmi_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_avnet_dp_nmi_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_avnet_dp_nmi_bnl = new fc_feed_avnet_dp_nmi_bnl_converter()
converterMap+=("fc_feed_avnet_dp_nmi_bnl"->dummyobj_fc_feed_avnet_dp_nmi_bnl)
class fc_feed_dbroberts_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_dbroberts(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_dbroberts = new fc_feed_dbroberts_converter()
converterMap+=("fc_feed_dbroberts"->dummyobj_fc_feed_dbroberts)
class fc_feed_chip1stop_jpy_oems_3_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_chip1stop_jpy_oems_3(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_chip1stop_jpy_oems_3 = new fc_feed_chip1stop_jpy_oems_3_converter()
converterMap+=("fc_feed_chip1stop_jpy_oems_3"->dummyobj_fc_feed_chip1stop_jpy_oems_3)
class fc_feed_wbchips_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_wbchips(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_wbchips = new fc_feed_wbchips_converter()
converterMap+=("fc_feed_wbchips"->dummyobj_fc_feed_wbchips)
class fc_feed_allied_oemstrade_ex_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_allied_oemstrade_ex(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_allied_oemstrade_ex = new fc_feed_allied_oemstrade_ex_converter()
converterMap+=("fc_feed_allied_oemstrade_ex"->dummyobj_fc_feed_allied_oemstrade_ex)
class fc_feed_diverse_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_diverse(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_diverse = new fc_feed_diverse_converter()
converterMap+=("fc_feed_diverse"->dummyobj_fc_feed_diverse)
class fc_feed_rochester_discounted_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rochester_discounted_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rochester_discounted_bnl = new fc_feed_rochester_discounted_bnl_converter()
converterMap+=("fc_feed_rochester_discounted_bnl"->dummyobj_fc_feed_rochester_discounted_bnl)
class fc_feed_samtec_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_samtec(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_samtec = new fc_feed_samtec_converter()
converterMap+=("fc_feed_samtec"->dummyobj_fc_feed_samtec)
class fc_feed_briocean_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_briocean(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_briocean = new fc_feed_briocean_converter()
converterMap+=("fc_feed_briocean"->dummyobj_fc_feed_briocean)
class fc_feed_kronex_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_kronex(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_kronex = new fc_feed_kronex_converter()
converterMap+=("fc_feed_kronex"->dummyobj_fc_feed_kronex)
class fc_feed_circular_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_circular(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_circular = new fc_feed_circular_converter()
converterMap+=("fc_feed_circular"->dummyobj_fc_feed_circular)
class fc_feed_farnell_uk_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_farnell_uk(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_farnell_uk = new fc_feed_farnell_uk_converter()
converterMap+=("fc_feed_farnell_uk"->dummyobj_fc_feed_farnell_uk)
class fc_feed_statemotor_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_statemotor(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_statemotor = new fc_feed_statemotor_converter()
converterMap+=("fc_feed_statemotor"->dummyobj_fc_feed_statemotor)
class fc_feed_b2b_wpg_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_b2b_wpg(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_b2b_wpg = new fc_feed_b2b_wpg_converter()
converterMap+=("fc_feed_b2b_wpg"->dummyobj_fc_feed_b2b_wpg)
class fc_feed_farnell_hu_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_farnell_hu_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_farnell_hu_bnl = new fc_feed_farnell_hu_bnl_converter()
converterMap+=("fc_feed_farnell_hu_bnl"->dummyobj_fc_feed_farnell_hu_bnl)
class fc_feed_waytek_feed_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_waytek_feed_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_waytek_feed_bnl = new fc_feed_waytek_feed_bnl_converter()
converterMap+=("fc_feed_waytek_feed_bnl"->dummyobj_fc_feed_waytek_feed_bnl)
class fc_feed_element14_bnl_littelfuse_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_element14_bnl_littelfuse(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_element14_bnl_littelfuse = new fc_feed_element14_bnl_littelfuse_converter()
converterMap+=("fc_feed_element14_bnl_littelfuse"->dummyobj_fc_feed_element14_bnl_littelfuse)
class fc_feed_nacsemi_findchips_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_nacsemi_findchips_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_nacsemi_findchips_bnl = new fc_feed_nacsemi_findchips_bnl_converter()
converterMap+=("fc_feed_nacsemi_findchips_bnl"->dummyobj_fc_feed_nacsemi_findchips_bnl)
class fc_feed_chipmall_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_chipmall(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_chipmall = new fc_feed_chipmall_converter()
converterMap+=("fc_feed_chipmall"->dummyobj_fc_feed_chipmall)
class fc_feed_suishen_oems_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_suishen_oems(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_suishen_oems = new fc_feed_suishen_oems_converter()
converterMap+=("fc_feed_suishen_oems"->dummyobj_fc_feed_suishen_oems)
class fc_feed_heilind_europe_oems_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_heilind_europe_oems(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_heilind_europe_oems = new fc_feed_heilind_europe_oems_converter()
converterMap+=("fc_feed_heilind_europe_oems"->dummyobj_fc_feed_heilind_europe_oems)
class fc_feed_iconline_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_iconline(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_iconline = new fc_feed_iconline_converter()
converterMap+=("fc_feed_iconline"->dummyobj_fc_feed_iconline)
class fc_feed_assetgreen_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_assetgreen(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_assetgreen = new fc_feed_assetgreen_converter()
converterMap+=("fc_feed_assetgreen"->dummyobj_fc_feed_assetgreen)
class fc_feed_rfpd_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rfpd(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rfpd = new fc_feed_rfpd_converter()
converterMap+=("fc_feed_rfpd"->dummyobj_fc_feed_rfpd)
class fc_feed_nacsemi_fc_us_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_nacsemi_fc_us(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_nacsemi_fc_us = new fc_feed_nacsemi_fc_us_converter()
converterMap+=("fc_feed_nacsemi_fc_us"->dummyobj_fc_feed_nacsemi_fc_us)
class fc_feed_teconn_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_teconn(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_teconn = new fc_feed_teconn_converter()
converterMap+=("fc_feed_teconn"->dummyobj_fc_feed_teconn)
class fc_feed_rs_components_ph_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_ph(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_ph = new fc_feed_rs_components_ph_converter()
converterMap+=("fc_feed_rs_components_ph"->dummyobj_fc_feed_rs_components_ph)
class fc_feed_dynamic_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_dynamic(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_dynamic = new fc_feed_dynamic_converter()
converterMap+=("fc_feed_dynamic"->dummyobj_fc_feed_dynamic)
class fc_feed_element14_nz_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_element14_nz(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_element14_nz = new fc_feed_element14_nz_converter()
converterMap+=("fc_feed_element14_nz"->dummyobj_fc_feed_element14_nz)
class fc_feed_schukat_fc_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_schukat_fc(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_schukat_fc = new fc_feed_schukat_fc_converter()
converterMap+=("fc_feed_schukat_fc"->dummyobj_fc_feed_schukat_fc)
class fc_feed_rutronik_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rutronik(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rutronik = new fc_feed_rutronik_converter()
converterMap+=("fc_feed_rutronik"->dummyobj_fc_feed_rutronik)
class fc_feed_rs_components_se_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_se(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_se = new fc_feed_rs_components_se_converter()
converterMap+=("fc_feed_rs_components_se"->dummyobj_fc_feed_rs_components_se)
class fc_feed_comsit_fc_es_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_comsit_fc_es(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_comsit_fc_es = new fc_feed_comsit_fc_es_converter()
converterMap+=("fc_feed_comsit_fc_es"->dummyobj_fc_feed_comsit_fc_es)
class fc_feed_maritex_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_maritex(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_maritex = new fc_feed_maritex_converter()
converterMap+=("fc_feed_maritex"->dummyobj_fc_feed_maritex)
class fc_feed_southelec_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_southelec(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_southelec = new fc_feed_southelec_converter()
converterMap+=("fc_feed_southelec"->dummyobj_fc_feed_southelec)
class fc_feed_newadvantage_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_newadvantage_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_newadvantage_bnl = new fc_feed_newadvantage_bnl_converter()
converterMap+=("fc_feed_newadvantage_bnl"->dummyobj_fc_feed_newadvantage_bnl)
class fc_feed_microchip_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_microchip(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_microchip = new fc_feed_microchip_converter()
converterMap+=("fc_feed_microchip"->dummyobj_fc_feed_microchip)
class fc_feed_ti_bnl_asc_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_ti_bnl_asc(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_ti_bnl_asc = new fc_feed_ti_bnl_asc_converter()
converterMap+=("fc_feed_ti_bnl_asc"->dummyobj_fc_feed_ti_bnl_asc)
class fc_feed_walkerind_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_walkerind(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_walkerind = new fc_feed_walkerind_converter()
converterMap+=("fc_feed_walkerind"->dummyobj_fc_feed_walkerind)
class fc_feed_libra_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_libra(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_libra = new fc_feed_libra_converter()
converterMap+=("fc_feed_libra"->dummyobj_fc_feed_libra)
class fc_feed_distrelec_sk_oemstrade_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_distrelec_sk_oemstrade(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_distrelec_sk_oemstrade = new fc_feed_distrelec_sk_oemstrade_converter()
converterMap+=("fc_feed_distrelec_sk_oemstrade"->dummyobj_fc_feed_distrelec_sk_oemstrade)
class fc_feed_mvcomp_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_mvcomp(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_mvcomp = new fc_feed_mvcomp_converter()
converterMap+=("fc_feed_mvcomp"->dummyobj_fc_feed_mvcomp)
class fc_feed_arrow_bnl_2_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_arrow_bnl_2(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_arrow_bnl_2 = new fc_feed_arrow_bnl_2_converter()
converterMap+=("fc_feed_arrow_bnl_2"->dummyobj_fc_feed_arrow_bnl_2)
class fc_feed_element14_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_element14(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_element14 = new fc_feed_element14_converter()
converterMap+=("fc_feed_element14"->dummyobj_fc_feed_element14)
class fc_feed_shengyu_fc_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_shengyu_fc(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_shengyu_fc = new fc_feed_shengyu_fc_converter()
converterMap+=("fc_feed_shengyu_fc"->dummyobj_fc_feed_shengyu_fc)
class fc_feed_peigenesis_003_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_peigenesis_003(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_peigenesis_003 = new fc_feed_peigenesis_003_converter()
converterMap+=("fc_feed_peigenesis_003"->dummyobj_fc_feed_peigenesis_003)
class fc_feed_terapart_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_terapart(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_terapart = new fc_feed_terapart_converter()
converterMap+=("fc_feed_terapart"->dummyobj_fc_feed_terapart)
class fc_feed_element14_vn_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_element14_vn(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_element14_vn = new fc_feed_element14_vn_converter()
converterMap+=("fc_feed_element14_vn"->dummyobj_fc_feed_element14_vn)
class fc_feed_jameco_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_jameco(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_jameco = new fc_feed_jameco_converter()
converterMap+=("fc_feed_jameco"->dummyobj_fc_feed_jameco)
class fc_feed_symmetry_oems_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_symmetry_oems(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_symmetry_oems = new fc_feed_symmetry_oems_converter()
converterMap+=("fc_feed_symmetry_oems"->dummyobj_fc_feed_symmetry_oems)
class fc_feed_distrelec_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_distrelec(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_distrelec = new fc_feed_distrelec_converter()
converterMap+=("fc_feed_distrelec"->dummyobj_fc_feed_distrelec)
class fc_feed_gallop_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_gallop(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_gallop = new fc_feed_gallop_converter()
converterMap+=("fc_feed_gallop"->dummyobj_fc_feed_gallop)
class fc_feed_heilind_americas_asia_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_heilind_americas_asia(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_heilind_americas_asia = new fc_feed_heilind_americas_asia_converter()
converterMap+=("fc_feed_heilind_americas_asia"->dummyobj_fc_feed_heilind_americas_asia)
class fc_feed_comsit_oems_mx_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_comsit_oems_mx(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_comsit_oems_mx = new fc_feed_comsit_oems_mx_converter()
converterMap+=("fc_feed_comsit_oems_mx"->dummyobj_fc_feed_comsit_oems_mx)
class fc_feed_rs_components_ru_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_ru(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_ru = new fc_feed_rs_components_ru_converter()
converterMap+=("fc_feed_rs_components_ru"->dummyobj_fc_feed_rs_components_ru)
class fc_feed_peigenesis_006_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_peigenesis_006(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_peigenesis_006 = new fc_feed_peigenesis_006_converter()
converterMap+=("fc_feed_peigenesis_006"->dummyobj_fc_feed_peigenesis_006)
class fc_feed_oneyac_fc_global_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_oneyac_fc_global(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_oneyac_fc_global = new fc_feed_oneyac_fc_global_converter()
converterMap+=("fc_feed_oneyac_fc_global"->dummyobj_fc_feed_oneyac_fc_global)
class fc_feed_digikey_hk_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_digikey_hk(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_digikey_hk = new fc_feed_digikey_hk_converter()
converterMap+=("fc_feed_digikey_hk"->dummyobj_fc_feed_digikey_hk)
class fc_feed_acds_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_acds(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_acds = new fc_feed_acds_converter()
converterMap+=("fc_feed_acds"->dummyobj_fc_feed_acds)
class fc_feed_rs_components_sk_mft_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_sk_mft_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_sk_mft_bnl = new fc_feed_rs_components_sk_mft_bnl_converter()
converterMap+=("fc_feed_rs_components_sk_mft_bnl"->dummyobj_fc_feed_rs_components_sk_mft_bnl)
class fc_feed_distrelec_sk_findchips_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_distrelec_sk_findchips(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_distrelec_sk_findchips = new fc_feed_distrelec_sk_findchips_converter()
converterMap+=("fc_feed_distrelec_sk_findchips"->dummyobj_fc_feed_distrelec_sk_findchips)
class fc_feed_tedss_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_tedss(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_tedss = new fc_feed_tedss_converter()
converterMap+=("fc_feed_tedss"->dummyobj_fc_feed_tedss)
class fc_feed_toponebuy_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_toponebuy(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_toponebuy = new fc_feed_toponebuy_converter()
converterMap+=("fc_feed_toponebuy"->dummyobj_fc_feed_toponebuy)
class fc_feed_arrow_china_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_arrow_china(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_arrow_china = new fc_feed_arrow_china_converter()
converterMap+=("fc_feed_arrow_china"->dummyobj_fc_feed_arrow_china)
class fc_feed_chipone_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_chipone(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_chipone = new fc_feed_chipone_converter()
converterMap+=("fc_feed_chipone"->dummyobj_fc_feed_chipone)
class fc_feed_glyn_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_glyn(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_glyn = new fc_feed_glyn_converter()
converterMap+=("fc_feed_glyn"->dummyobj_fc_feed_glyn)
class fc_feed_enteermall_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_enteermall(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_enteermall = new fc_feed_enteermall_converter()
converterMap+=("fc_feed_enteermall"->dummyobj_fc_feed_enteermall)
class fc_feed_rcelec_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rcelec(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rcelec = new fc_feed_rcelec_converter()
converterMap+=("fc_feed_rcelec"->dummyobj_fc_feed_rcelec)
class fc_feed_utmel_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_utmel(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_utmel = new fc_feed_utmel_converter()
converterMap+=("fc_feed_utmel"->dummyobj_fc_feed_utmel)
class fc_feed_avnet_asia_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_avnet_asia_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_avnet_asia_bnl = new fc_feed_avnet_asia_bnl_converter()
converterMap+=("fc_feed_avnet_asia_bnl"->dummyobj_fc_feed_avnet_asia_bnl)
class fc_feed_inductors_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_inductors(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_inductors = new fc_feed_inductors_converter()
converterMap+=("fc_feed_inductors"->dummyobj_fc_feed_inductors)
class fc_feed_tencell_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_tencell(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_tencell = new fc_feed_tencell_converter()
converterMap+=("fc_feed_tencell"->dummyobj_fc_feed_tencell)
class fc_feed_acmechip_oems_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_acmechip_oems(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_acmechip_oems = new fc_feed_acmechip_oems_converter()
converterMap+=("fc_feed_acmechip_oems"->dummyobj_fc_feed_acmechip_oems)
class fc_feed_rspro_nz_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rspro_nz(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rspro_nz = new fc_feed_rspro_nz_converter()
converterMap+=("fc_feed_rspro_nz"->dummyobj_fc_feed_rspro_nz)
class fc_feed_mps_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_mps(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_mps = new fc_feed_mps_converter()
converterMap+=("fc_feed_mps"->dummyobj_fc_feed_mps)
class fc_feed_digikey_uk_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_digikey_uk_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_digikey_uk_bnl = new fc_feed_digikey_uk_bnl_converter()
converterMap+=("fc_feed_digikey_uk_bnl"->dummyobj_fc_feed_digikey_uk_bnl)
class fc_feed_era_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_era(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_era = new fc_feed_era_converter()
converterMap+=("fc_feed_era"->dummyobj_fc_feed_era)
class fc_feed_ameya_fc_cn_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_ameya_fc_cn(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_ameya_fc_cn = new fc_feed_ameya_fc_cn_converter()
converterMap+=("fc_feed_ameya_fc_cn"->dummyobj_fc_feed_ameya_fc_cn)
class fc_feed_rochester_kr_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rochester_kr(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rochester_kr = new fc_feed_rochester_kr_converter()
converterMap+=("fc_feed_rochester_kr"->dummyobj_fc_feed_rochester_kr)
class fc_feed_ukserfala_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_ukserfala(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_ukserfala = new fc_feed_ukserfala_converter()
converterMap+=("fc_feed_ukserfala"->dummyobj_fc_feed_ukserfala)
class fc_feed_coilcraft_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_coilcraft_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_coilcraft_bnl = new fc_feed_coilcraft_bnl_converter()
converterMap+=("fc_feed_coilcraft_bnl"->dummyobj_fc_feed_coilcraft_bnl)
class fc_feed_cytech_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_cytech(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_cytech = new fc_feed_cytech_converter()
converterMap+=("fc_feed_cytech"->dummyobj_fc_feed_cytech)
class fc_feed_apvm_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_apvm(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_apvm = new fc_feed_apvm_converter()
converterMap+=("fc_feed_apvm"->dummyobj_fc_feed_apvm)
class fc_feed_chip1stop_cny_oems_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_chip1stop_cny_oems(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_chip1stop_cny_oems = new fc_feed_chip1stop_cny_oems_converter()
converterMap+=("fc_feed_chip1stop_cny_oems"->dummyobj_fc_feed_chip1stop_cny_oems)
class fc_feed_farnell_tr_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_farnell_tr(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_farnell_tr = new fc_feed_farnell_tr_converter()
converterMap+=("fc_feed_farnell_tr"->dummyobj_fc_feed_farnell_tr)
class fc_feed_rs_components_pt_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_pt(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_pt = new fc_feed_rs_components_pt_converter()
converterMap+=("fc_feed_rs_components_pt"->dummyobj_fc_feed_rs_components_pt)
class fc_feed_taprobain_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_taprobain(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_taprobain = new fc_feed_taprobain_converter()
converterMap+=("fc_feed_taprobain"->dummyobj_fc_feed_taprobain)
class fc_feed_distrelec_hu_oemstrade_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_distrelec_hu_oemstrade(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_distrelec_hu_oemstrade = new fc_feed_distrelec_hu_oemstrade_converter()
converterMap+=("fc_feed_distrelec_hu_oemstrade"->dummyobj_fc_feed_distrelec_hu_oemstrade)
class fc_feed_comsit_oems_ru_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_comsit_oems_ru(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_comsit_oems_ru = new fc_feed_comsit_oems_ru_converter()
converterMap+=("fc_feed_comsit_oems_ru"->dummyobj_fc_feed_comsit_oems_ru)
class fc_feed_rs_components_maxim_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_maxim_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_maxim_bnl = new fc_feed_rs_components_maxim_bnl_converter()
converterMap+=("fc_feed_rs_components_maxim_bnl"->dummyobj_fc_feed_rs_components_maxim_bnl)
class fc_feed_comsit_fc_fr_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_comsit_fc_fr(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_comsit_fc_fr = new fc_feed_comsit_fc_fr_converter()
converterMap+=("fc_feed_comsit_fc_fr"->dummyobj_fc_feed_comsit_fc_fr)
class fc_feed_allied_findchips_ex_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_allied_findchips_ex(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_allied_findchips_ex = new fc_feed_allied_findchips_ex_converter()
converterMap+=("fc_feed_allied_findchips_ex"->dummyobj_fc_feed_allied_findchips_ex)
class fc_feed_icsole_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_icsole(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_icsole = new fc_feed_icsole_converter()
converterMap+=("fc_feed_icsole"->dummyobj_fc_feed_icsole)
class fc_feed_shortec_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_shortec(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_shortec = new fc_feed_shortec_converter()
converterMap+=("fc_feed_shortec"->dummyobj_fc_feed_shortec)
class fc_feed_cisemiconductors_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_cisemiconductors(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_cisemiconductors = new fc_feed_cisemiconductors_converter()
converterMap+=("fc_feed_cisemiconductors"->dummyobj_fc_feed_cisemiconductors)
class fc_feed_farnell_il_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_farnell_il(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_farnell_il = new fc_feed_farnell_il_converter()
converterMap+=("fc_feed_farnell_il"->dummyobj_fc_feed_farnell_il)
class fc_feed_heilind_asia_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_heilind_asia(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_heilind_asia = new fc_feed_heilind_asia_converter()
converterMap+=("fc_feed_heilind_asia"->dummyobj_fc_feed_heilind_asia)
class fc_feed_supertronic_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_supertronic(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_supertronic = new fc_feed_supertronic_converter()
converterMap+=("fc_feed_supertronic"->dummyobj_fc_feed_supertronic)
class fc_feed_distrelec_ro_oemstrade_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_distrelec_ro_oemstrade(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_distrelec_ro_oemstrade = new fc_feed_distrelec_ro_oemstrade_converter()
converterMap+=("fc_feed_distrelec_ro_oemstrade"->dummyobj_fc_feed_distrelec_ro_oemstrade)
class fc_feed_heqingelec_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_heqingelec(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_heqingelec = new fc_feed_heqingelec_converter()
converterMap+=("fc_feed_heqingelec"->dummyobj_fc_feed_heqingelec)
class fc_feed_hsictrading_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_hsictrading(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_hsictrading = new fc_feed_hsictrading_converter()
converterMap+=("fc_feed_hsictrading"->dummyobj_fc_feed_hsictrading)
class fc_feed_arrow_bnl_pemco_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_arrow_bnl_pemco(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_arrow_bnl_pemco = new fc_feed_arrow_bnl_pemco_converter()
converterMap+=("fc_feed_arrow_bnl_pemco"->dummyobj_fc_feed_arrow_bnl_pemco)
class fc_feed_goodiclink_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_goodiclink(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_goodiclink = new fc_feed_goodiclink_converter()
converterMap+=("fc_feed_goodiclink"->dummyobj_fc_feed_goodiclink)
class fc_feed_hsmelect_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_hsmelect(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_hsmelect = new fc_feed_hsmelect_converter()
converterMap+=("fc_feed_hsmelect"->dummyobj_fc_feed_hsmelect)
class fc_feed_ti_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_ti_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_ti_bnl = new fc_feed_ti_bnl_converter()
converterMap+=("fc_feed_ti_bnl"->dummyobj_fc_feed_ti_bnl)
class fc_feed_smrelec_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_smrelec(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_smrelec = new fc_feed_smrelec_converter()
converterMap+=("fc_feed_smrelec"->dummyobj_fc_feed_smrelec)
class fc_feed_rochester_jp_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rochester_jp(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rochester_jp = new fc_feed_rochester_jp_converter()
converterMap+=("fc_feed_rochester_jp"->dummyobj_fc_feed_rochester_jp)
class fc_feed_winsource_cse_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_winsource_cse(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_winsource_cse = new fc_feed_winsource_cse_converter()
converterMap+=("fc_feed_winsource_cse"->dummyobj_fc_feed_winsource_cse)
class fc_feed_lvy_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_lvy(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_lvy = new fc_feed_lvy_converter()
converterMap+=("fc_feed_lvy"->dummyobj_fc_feed_lvy)
class fc_feed_chip1stop_jpy_multi_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_chip1stop_jpy_multi(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_chip1stop_jpy_multi = new fc_feed_chip1stop_jpy_multi_converter()
converterMap+=("fc_feed_chip1stop_jpy_multi"->dummyobj_fc_feed_chip1stop_jpy_multi)
class fc_feed_distrelec_hu_findchips_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_distrelec_hu_findchips(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_distrelec_hu_findchips = new fc_feed_distrelec_hu_findchips_converter()
converterMap+=("fc_feed_distrelec_hu_findchips"->dummyobj_fc_feed_distrelec_hu_findchips)
class fc_feed_fanco_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_fanco(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_fanco = new fc_feed_fanco_converter()
converterMap+=("fc_feed_fanco"->dummyobj_fc_feed_fanco)
class fc_feed_rs_components_au_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rs_components_au(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rs_components_au = new fc_feed_rs_components_au_converter()
converterMap+=("fc_feed_rs_components_au"->dummyobj_fc_feed_rs_components_au)
class fc_feed_eve_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_eve(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_eve = new fc_feed_eve_converter()
converterMap+=("fc_feed_eve"->dummyobj_fc_feed_eve)
class fc_feed_teltek_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_teltek(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_teltek = new fc_feed_teltek_converter()
converterMap+=("fc_feed_teltek"->dummyobj_fc_feed_teltek)
class fc_feed_technitool_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_technitool(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_technitool = new fc_feed_technitool_converter()
converterMap+=("fc_feed_technitool"->dummyobj_fc_feed_technitool)
class fc_feed_ardusimple_oems_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_ardusimple_oems(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_ardusimple_oems = new fc_feed_ardusimple_oems_converter()
converterMap+=("fc_feed_ardusimple_oems"->dummyobj_fc_feed_ardusimple_oems)
class fc_feed_acmechip_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_acmechip(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_acmechip = new fc_feed_acmechip_converter()
converterMap+=("fc_feed_acmechip"->dummyobj_fc_feed_acmechip)
class fc_feed_ntd_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_ntd(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_ntd = new fc_feed_ntd_converter()
converterMap+=("fc_feed_ntd"->dummyobj_fc_feed_ntd)
class fc_feed_kailiyuan_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_kailiyuan(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_kailiyuan = new fc_feed_kailiyuan_converter()
converterMap+=("fc_feed_kailiyuan"->dummyobj_fc_feed_kailiyuan)
class fc_feed_handchain_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_handchain(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_handchain = new fc_feed_handchain_converter()
converterMap+=("fc_feed_handchain"->dummyobj_fc_feed_handchain)
class fc_feed_farnell_rohde_bnl_uk_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_farnell_rohde_bnl_uk(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_farnell_rohde_bnl_uk = new fc_feed_farnell_rohde_bnl_uk_converter()
converterMap+=("fc_feed_farnell_rohde_bnl_uk"->dummyobj_fc_feed_farnell_rohde_bnl_uk)
class fc_feed_conrad_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_conrad(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_conrad = new fc_feed_conrad_converter()
converterMap+=("fc_feed_conrad"->dummyobj_fc_feed_conrad)
class fc_feed_chip1stop_jpy_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_chip1stop_jpy(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_chip1stop_jpy = new fc_feed_chip1stop_jpy_converter()
converterMap+=("fc_feed_chip1stop_jpy"->dummyobj_fc_feed_chip1stop_jpy)
class fc_feed_distrelec_nl_oemstrade_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_distrelec_nl_oemstrade(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_distrelec_nl_oemstrade = new fc_feed_distrelec_nl_oemstrade_converter()
converterMap+=("fc_feed_distrelec_nl_oemstrade"->dummyobj_fc_feed_distrelec_nl_oemstrade)
class fc_feed_rspro_pl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rspro_pl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rspro_pl = new fc_feed_rspro_pl_converter()
converterMap+=("fc_feed_rspro_pl"->dummyobj_fc_feed_rspro_pl)
class fc_feed_olc_oems_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_olc_oems(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_olc_oems = new fc_feed_olc_oems_converter()
converterMap+=("fc_feed_olc_oems"->dummyobj_fc_feed_olc_oems)
class fc_feed_newark_mx_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_newark_mx(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_newark_mx = new fc_feed_newark_mx_converter()
converterMap+=("fc_feed_newark_mx"->dummyobj_fc_feed_newark_mx)
class fc_feed_liangxin_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_liangxin(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_liangxin = new fc_feed_liangxin_converter()
converterMap+=("fc_feed_liangxin"->dummyobj_fc_feed_liangxin)
class fc_feed_suntronic_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_suntronic(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_suntronic = new fc_feed_suntronic_converter()
converterMap+=("fc_feed_suntronic"->dummyobj_fc_feed_suntronic)
class fc_feed_cdi_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_cdi(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_cdi = new fc_feed_cdi_converter()
converterMap+=("fc_feed_cdi"->dummyobj_fc_feed_cdi)
class fc_feed_chip1stop_usd_bnl_test_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_chip1stop_usd_bnl_test(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_chip1stop_usd_bnl_test = new fc_feed_chip1stop_usd_bnl_test_converter()
converterMap+=("fc_feed_chip1stop_usd_bnl_test"->dummyobj_fc_feed_chip1stop_usd_bnl_test)
class fc_feed_element14_hk_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_element14_hk(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_element14_hk = new fc_feed_element14_hk_converter()
converterMap+=("fc_feed_element14_hk"->dummyobj_fc_feed_element14_hk)
class fc_feed_chip1stop_cny_oems_2_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_chip1stop_cny_oems_2(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_chip1stop_cny_oems_2 = new fc_feed_chip1stop_cny_oems_2_converter()
converterMap+=("fc_feed_chip1stop_cny_oems_2"->dummyobj_fc_feed_chip1stop_cny_oems_2)
class fc_feed_venkel_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_venkel(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_venkel = new fc_feed_venkel_converter()
converterMap+=("fc_feed_venkel"->dummyobj_fc_feed_venkel)
class fc_feed_smartpioneer_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_smartpioneer(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_smartpioneer = new fc_feed_smartpioneer_converter()
converterMap+=("fc_feed_smartpioneer"->dummyobj_fc_feed_smartpioneer)
class fc_feed_interine_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_interine(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_interine = new fc_feed_interine_converter()
converterMap+=("fc_feed_interine"->dummyobj_fc_feed_interine)
class fc_feed_ickey_china_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_ickey_china(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_ickey_china = new fc_feed_ickey_china_converter()
converterMap+=("fc_feed_ickey_china"->dummyobj_fc_feed_ickey_china)
class fc_feed_rochester_mx_bnl_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_rochester_mx_bnl(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_rochester_mx_bnl = new fc_feed_rochester_mx_bnl_converter()
converterMap+=("fc_feed_rochester_mx_bnl"->dummyobj_fc_feed_rochester_mx_bnl)
class fc_feed_teconn_bnl_2_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_teconn_bnl_2(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_teconn_bnl_2 = new fc_feed_teconn_bnl_2_converter()
converterMap+=("fc_feed_teconn_bnl_2"->dummyobj_fc_feed_teconn_bnl_2)
class fc_feed_techdesign_bnl_winbond_converter extends Serializable{
  def convert(inpath:String):DataFrame={
    val lines = spark.read.format("text").option("delimiter","\t").load(inpath)
    val ds = lines.map(line=>{
      val oldfields = line(0).toString.split("\t")
      var fields = oldfields
      if(oldfields.length<44) fields = oldfields.padTo(44,null)
      class_fc_feed_techdesign_bnl_winbond(str2int(fields(0)),str2int(fields(1)),checknull(fields(2)),checknull(fields(3)),checknull(fields(4)),checknull(fields(5)),checknull(fields(6)),str2long(fields(7)),checknull(fields(8)),checknull(fields(9)),checknull(fields(10)),checknull(fields(11)),checknull(fields(12)),checknull(fields(13)),checknull(fields(14)),checknull(fields(15)),checknull(fields(16)),checknull(fields(17)),checknull(fields(18)),checknull(fields(19)),checknull(fields(20)),checknull(fields(21)),checknull(fields(22)),checknull(fields(23)),str2int(fields(24)),checknull(fields(25)),checknull(fields(26)),checknull(fields(27)),str2int(fields(28)),str2int(fields(29)),checknull(fields(30)),checknull(fields(31)),str2int(fields(32)),checknull(fields(33)),checknull(fields(34)),checknull(fields(35)),checknull(fields(36)),checknull(fields(37)),checknull(fields(38)),checknull(fields(39)),checknull(fields(40)),checknull(fields(41)),checknull(fields(42)),str2long(fields(43)))    })
    ds.toDF()
  }
}
val dummyobj_fc_feed_techdesign_bnl_winbond = new fc_feed_techdesign_bnl_winbond_converter()
converterMap+=("fc_feed_techdesign_bnl_winbond"->dummyobj_fc_feed_techdesign_bnl_winbond)
